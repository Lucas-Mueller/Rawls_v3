{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d36bcd1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d6623a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import random\n",
    "import yaml\n",
    "# Change to project root directory\n",
    "project_root = '/Users/lucasmuller/Desktop/Githubg/Rawls_v3'\n",
    "os.chdir(project_root)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils.experiment_runner import generate_random_config, generate_and_save_configs, run_experiment, run_experiments_parallel\n",
    "\n",
    "from typing import Union, List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d1ece",
   "metadata": {},
   "source": [
    "# Parameter Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d50d92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"x-ai/grok-4\",\n",
    "    \"google/gemini-2.5-flash\",\n",
    "    \"google/gemini-2.5-pro\",\n",
    "    \"meta-llama/llama-4-maverick\",\n",
    "    \"anthropic/claude-sonnet-4\",\n",
    "    \"openai/gpt-4.1-mini\",\n",
    "    \"openai/gpt-4.1\",\n",
    "    \"deepseek/deepseek-chat-v3-0324\",\n",
    "    \"qwen/qwen3-coder\",\n",
    "    \"moonshotai/kimi-k2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [\n",
    "    \"openai/gpt-4.1-nano\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e06703f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_config(\n",
    "      num_agents: int = 3,\n",
    "      num_rounds: int = 5,\n",
    "      personality: str = \"You are an american college student.\",\n",
    "      models: Union[str, List[str]] = models,\n",
    "      temperature: Union[float, tuple] = (0.0, 1.0),\n",
    "      memory_limit: int = 50000,\n",
    "      reasoning_enabled: bool = True,\n",
    "      utility_model: str = \"openai/gpt-4.1-nano\",\n",
    "      language: str = \"English\",\n",
    "      distribution_range_phase1: List[float] = [0.5, 2.0],\n",
    "      distribution_range_phase2: List[float] = [0.5, 2.0]\n",
    "  ) -> Dict[str, Any]:\n",
    "      \"\"\"\n",
    "      Generate a random configuration for the Frohlich Experiment.\n",
    "      \n",
    "      Parameters:\n",
    "      - num_agents: Number of participant agents\n",
    "      - num_rounds: Number of Phase 2 rounds\n",
    "      - personality: Single personality description used for all agents\n",
    "      - models: Single model string or list to randomly select from\n",
    "      - temperature: Fixed temperature (float) or range (tuple) for random selection\n",
    "      - memory_limit: Memory character limit for agents\n",
    "      - reasoning_enabled: Whether agents use reasoning\n",
    "      - utility_model: Model for utility agent\n",
    "      - language: Language for prompts (\"English\", \"Spanish\", \"Mandarin\")\n",
    "      - distribution_range_phase1/2: Income distribution multiplier ranges\n",
    "      \n",
    "      Returns:\n",
    "      - Dictionary representing the experiment configuration\n",
    "      \"\"\"\n",
    "\n",
    "      # Generate agents\n",
    "      agents = []\n",
    "      for i in range(num_agents):\n",
    "          # Generate agent name as Agent_1, Agent_2, etc.\n",
    "          agent_name = f\"Agent_{i + 1}\"\n",
    "\n",
    "          # Select model\n",
    "          if isinstance(models, list):\n",
    "              selected_model = random.choice(models)\n",
    "          else:\n",
    "              selected_model = models\n",
    "\n",
    "          # Select temperature\n",
    "          if isinstance(temperature, tuple) and len(temperature) == 2:\n",
    "              selected_temp = round(random.uniform(temperature[0], temperature[1]), 2)\n",
    "          else:\n",
    "              selected_temp = float(temperature)\n",
    "\n",
    "          agent_config = {\n",
    "              \"name\": agent_name,\n",
    "              \"personality\": personality,\n",
    "              \"model\": selected_model,\n",
    "              \"temperature\": selected_temp,\n",
    "              \"memory_character_limit\": memory_limit,\n",
    "              \"reasoning_enabled\": reasoning_enabled\n",
    "          }\n",
    "          agents.append(agent_config)\n",
    "\n",
    "      # Build complete configuration\n",
    "      config = {\n",
    "          \"language\": language,\n",
    "          \"agents\": agents,\n",
    "          \"utility_agent_model\": utility_model,\n",
    "          \"phase2_rounds\": num_rounds,\n",
    "          \"distribution_range_phase1\": distribution_range_phase1,\n",
    "          \"distribution_range_phase2\": distribution_range_phase2\n",
    "      }\n",
    "\n",
    "      return config\n",
    "\n",
    "def generate_and_save_configs(\n",
    "      num_configs: int = 10,\n",
    "      save_path: str = \"hypothesis_2_&_4/configs/condition_1\",\n",
    "      **config_params\n",
    "  ):\n",
    "      \"\"\"\n",
    "      Generate multiple random configurations and save them as YAML files.\n",
    "      \n",
    "      Parameters:\n",
    "      - num_configs: Number of configurations to generate\n",
    "      - save_path: Path to save the configuration files\n",
    "      - **config_params: Parameters to pass to generate_random_config()\n",
    "      \n",
    "      Returns:\n",
    "      - List of generated configurations\n",
    "      \"\"\"\n",
    "\n",
    "      # Create directory if it doesn't exist\n",
    "      os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "      configs = []\n",
    "      for i in range(num_configs):\n",
    "          # Generate random config\n",
    "          config = generate_random_config(**config_params)\n",
    "          configs.append(config)\n",
    "\n",
    "          # Save to YAML file\n",
    "          filename = f\"config_{i+1:02d}.yaml\"\n",
    "          filepath = os.path.join(save_path, filename)\n",
    "\n",
    "          with open(filepath, 'w') as f:\n",
    "              yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "          print(f\"Saved {filename}\")\n",
    "\n",
    "      print(f\"Generated and saved {num_configs} configurations in {save_path}\")\n",
    "      return configs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe746ee",
   "metadata": {},
   "source": [
    "# 1. Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c5e4f",
   "metadata": {},
   "source": [
    "## Generating Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7773f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved config_01.yaml\n",
      "Saved config_02.yaml\n",
      "Saved config_03.yaml\n",
      "Saved config_04.yaml\n",
      "Saved config_05.yaml\n",
      "Saved config_06.yaml\n",
      "Saved config_07.yaml\n",
      "Saved config_08.yaml\n",
      "Saved config_09.yaml\n",
      "Saved config_10.yaml\n",
      "Generated and saved 10 configurations in hypothesis_2_&_4/configs/condition_1\n"
     ]
    }
   ],
   "source": [
    "# Generate configs 1. \n",
    "# Commenteed out to avoid running if accidentally clicked run all \n",
    "configs = generate_and_save_configs(\n",
    "    num_configs=10,\n",
    "    num_agents=5,\n",
    "    personality=\"You are an american college student.\",\n",
    "    models=models,\n",
    "    temperature=(0),\n",
    "    num_rounds=5,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ba09a",
   "metadata": {},
   "source": [
    "## Running Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ef0d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 configuration files:\n",
      "   1. config_01.yaml\n",
      "   2. config_02.yaml\n",
      "   3. config_03.yaml\n",
      "   4. config_04.yaml\n",
      "\n",
      "Total configurations to run: 4\n"
     ]
    }
   ],
   "source": [
    "# Get all config files from condition_1\n",
    "config_dir = \"hypothesis_2_&_4/configs/condition_1\"\n",
    "config_files = sorted(glob.glob(f\"{config_dir}/config_*.yaml\"))\n",
    "\n",
    "print(f\"Found {len(config_files)} configuration files:\")\n",
    "for i, config_file in enumerate(config_files, 1):\n",
    "    config_name = Path(config_file).name\n",
    "    print(f\"  {i:2d}. {config_name}\")\n",
    "\n",
    "# Display the total\n",
    "print(f\"\\nTotal configurations to run: {len(config_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1e78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90c1784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel execution of 4 experiments\n",
      "Maximum parallel experiments: 2\n",
      "Output directory: hypothesis_2_&_4/logs\n",
      "Start time: 2025-08-11 01:50:08\n",
      "\n",
      "================================================================================\n",
      "\n",
      "2025-08-11 01:50:08,675 - utils.experiment_runner - INFO - Loaded 4 configurations\n",
      "2025-08-11 01:50:08,676 - utils.experiment_runner - INFO - Running with max 2 parallel experiments\n",
      "2025-08-11 01:50:08,676 - utils.experiment_runner - INFO - Starting parallel experiment execution...\n",
      "2025-08-11 01:50:08,677 - utils.experiment_runner - INFO - Starting experiment: config_01\n",
      "2025-08-11 01:50:08,681 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-11 01:50:08,682 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:50:08,682 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:50:08,683 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:50:08,683 - core.experiment_manager - INFO - Starting Phase 1 for experiment b0503597-6011-4c91-85f9-21d22e322c94\n",
      "2025-08-11 01:50:08,684 - utils.experiment_runner - INFO - Starting experiment: config_02\n",
      "2025-08-11 01:50:08,689 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-11 01:50:08,690 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:50:08,690 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:50:08,690 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:50:08,690 - core.experiment_manager - INFO - Starting Phase 1 for experiment b6e7087d-d783-4e50-b879-ed0825c8d048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:08,700 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:08,700 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:08,700 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:08,701 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:08,701 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:08,701 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:09,349 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:09,350 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:09,364 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:09,438 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:09,620 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:09,696 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:10,449 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:10,478 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:10,838 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:10,894 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:11,188 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:11,286 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:11,569 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:11,719 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:11,838 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:12,118 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:12,139 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:12,143 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:12,251 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:12,492 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:12,852 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:12,890 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:13,222 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:13,284 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:13,421 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:13,680 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:13,863 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:14,015 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:14,619 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:15,049 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:15,051 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:15,300 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:15,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:15,787 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:15,982 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:16,383 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:16,767 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:17,186 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:17,745 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:17,808 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:18,169 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:18,173 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:18,621 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:18,862 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:19,050 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:19,051 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:19,177 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:19,358 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:19,410 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:19,479 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:19,810 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:19,845 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:21,188 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:21,251 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:21,516 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:21,563 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:21,594 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:21,753 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:21,763 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:21,956 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:22,054 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:22,177 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:22,273 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:22,275 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:22,671 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:22,738 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:23,220 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:23,376 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:23,622 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:23,639 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:23,758 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:24,059 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:24,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:24,439 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:24,487 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:24,648 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:24,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:25,050 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:25,154 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:25,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:25,465 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:25,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:25,814 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:26,015 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:26,214 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:26,348 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:26,617 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:26,630 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:27,106 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:27,354 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:27,529 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:27,534 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:28,002 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:28,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:28,380 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:28,592 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:28,609 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:28,934 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:28,941 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:29,005 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:29,115 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:29,344 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:30,207 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:30,236 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:30,495 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:30,606 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:30,649 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:31,042 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:31,118 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:31,397 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:31,451 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:31,570 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:32,074 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:32,102 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:32,463 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:32,504 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:33,073 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:33,497 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:33,636 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:33,950 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:34,017 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:34,123 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:34,145 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:34,361 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:34,572 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:34,587 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:34,760 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:35,040 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:35,566 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:35,747 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:35,805 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:35,942 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:36,110 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:36,165 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:36,210 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:36,603 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:36,603 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:36,609 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:36,964 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:37,064 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:37,480 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:37,610 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:37,880 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:37,934 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:38,064 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:38,411 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:38,883 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:39,119 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:39,303 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:39,356 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:39,701 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:39,722 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:39,905 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:40,122 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:40,134 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:40,305 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:40,421 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:40,556 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:40,706 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:40,831 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:40,842 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:40,856 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:41,249 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:41,316 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:41,318 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:41,731 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:42,446 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:42,629 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:42,722 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:42,890 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:43,072 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:43,259 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:43,376 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:43,668 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:44,202 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:44,686 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:44,699 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:44,760 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:45,135 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:46,391 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:46,590 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:46,594 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:46,783 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:46,965 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:46,966 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:46,986 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:47,557 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:47,839 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:48,020 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:48,231 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:49,018 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:49,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:50,154 - core.experiment_manager - INFO - Phase 1 completed. 3 participants finished.\n",
      "2025-08-11 01:50:50,155 - core.experiment_manager - INFO - Agent_1: $7.36 earned\n",
      "2025-08-11 01:50:50,155 - core.experiment_manager - INFO - Agent_2: $7.92 earned\n",
      "2025-08-11 01:50:50,155 - core.experiment_manager - INFO - Agent_3: $7.67 earned\n",
      "2025-08-11 01:50:50,156 - core.experiment_manager - INFO - Starting Phase 2 for experiment b0503597-6011-4c91-85f9-21d22e322c94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:50,162 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:50,764 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:51,389 - core.experiment_manager - INFO - Phase 1 completed. 3 participants finished.\n",
      "2025-08-11 01:50:51,389 - core.experiment_manager - INFO - Agent_1: $7.59 earned\n",
      "2025-08-11 01:50:51,389 - core.experiment_manager - INFO - Agent_2: $11.29 earned\n",
      "2025-08-11 01:50:51,389 - core.experiment_manager - INFO - Agent_3: $5.87 earned\n",
      "2025-08-11 01:50:51,390 - core.experiment_manager - INFO - Starting Phase 2 for experiment b6e7087d-d783-4e50-b879-ed0825c8d048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:51,391 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:51,873 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:52,756 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:52,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:53,132 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:53,186 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:53,628 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:53,709 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:54,127 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:54,221 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:55,359 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:55,513 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:55,790 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:55,909 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:56,028 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:50:56,029 - core.phase2_manager - INFO - Agent: Agent_3\n",
      "2025-08-11 01:50:56,029 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint. I think this approach balances fairness and efficiency by ensuring everyones income stays within a reasonable range while still aiming for higher overall prosperity. I believe this could lead to a fairer outcome for everyone. \n",
      "\n",
      "My current preferred principle is the range constraint.\n",
      "2025-08-11 01:50:56,029 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:50:56,029 - core.phase2_manager - INFO - Vote proposal text: Vote on the principle of maximizing the average income with a range constraint\n",
      "2025-08-11 01:50:56,029 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,033 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,033 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,033 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:56,036 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:50:56,037 - core.phase2_manager - INFO - Agent: Agent_3\n",
      "2025-08-11 01:50:56,037 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint, to balance overall growth with fairness by limiting disparities. I believe this approach aligns with our shared interest in promoting equitable progress.\n",
      "2025-08-11 01:50:56,037 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:50:56,038 - core.phase2_manager - INFO - Vote proposal text: Vote on the principle of maximizing the average income with a range constraint to balance growth and fairness.\n",
      "2025-08-11 01:50:56,038 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,041 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,042 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,042 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:56,413 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:56,425 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:56,427 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:56,480 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:56,516 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:50:56,516 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:50:56,517 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:50:56,517 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:50:56,518 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:50:56,518 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:50:56,519 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,523 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,524 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,525 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:56,555 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:56,722 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:56,779 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:50:56,780 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:50:56,781 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:50:56,781 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:50:56,782 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:50:56,782 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:50:56,782 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,787 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,788 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:56,789 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:56,903 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:56,933 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:57,045 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:57,189 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:57,270 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:57,304 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:57,324 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:57,522 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:57,770 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:50:57,888 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:50:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:50:59,005 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:50:59,400 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:00,373 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:00,721 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:01,424 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:02,236 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:03,246 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:03,665 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:05,009 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:05,313 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:05,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:05,819 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:06,825 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:07,347 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:07,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:07,716 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:08,068 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:08,089 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:08,569 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:09,074 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:10,921 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:11,333 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:11,451 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:11,562 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:11,563 - core.phase2_manager - INFO - Agent: Agent_2\n",
      "2025-08-11 01:51:11,563 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint. I believe this approach effectively balances the goal of increasing overall wealth while preventing extreme disparities, which aligns with my values of fairness and efficiency. \n",
      "\n",
      "My current preferred principle is the range constraint.\n",
      "2025-08-11 01:51:11,563 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:11,563 - core.phase2_manager - INFO - Vote proposal text: Vote on the principle of maximizing the average income with a range constraint\n",
      "2025-08-11 01:51:11,563 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:11,566 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:11,566 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:11,566 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:11,699 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:11,898 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:11,899 - core.phase2_manager - INFO - Agent: Agent_1\n",
      "2025-08-11 01:51:11,899 - core.phase2_manager - INFO - Statement: I propose we vote on maximizing the average income with a range constraint, to ensure we promote economic growth while keeping disparities within a reasonable limit. I believe this approach best balances fairness and efficiency, which aligns with my values as a student concerned about societal stability.\n",
      "2025-08-11 01:51:11,899 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:11,900 - core.phase2_manager - INFO - Vote proposal text: Vote on maximizing the average income with a range constraint to balance economic growth and disparities.\n",
      "2025-08-11 01:51:11,900 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:11,902 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:11,903 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:11,903 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:11,945 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:12,007 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:12,076 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:12,133 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:12,133 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:12,133 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:12,134 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:12,134 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:51:12,134 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:51:12,134 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:12,136 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:12,137 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:12,137 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:12,310 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:12,315 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:12,316 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:12,395 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:12,396 - core.phase2_manager - INFO - Agent_1 response: 'NO' -> Contains YES: False\n",
      "2025-08-11 01:51:12,396 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:12,396 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:12,396 - core.phase2_manager - INFO - All agreements: [False, True, True]\n",
      "2025-08-11 01:51:12,397 - core.phase2_manager - INFO - Unanimous result: False\n",
      "2025-08-11 01:51:12,397 - core.phase2_manager - INFO - Unanimous agreement result: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:12,398 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:12,560 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:12,631 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:12,722 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:12,879 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:13,155 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:13,489 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:15,024 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:15,454 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:16,073 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:16,456 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:16,875 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:17,299 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:18,605 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:19,111 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:20,459 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:20,491 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:20,884 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:20,961 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:21,015 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:21,015 - core.phase2_manager - INFO - Agent: Agent_2\n",
      "2025-08-11 01:51:21,015 - core.phase2_manager - INFO - Statement: I propose we vote on maximizing the average income with a range constraint, to ensure we promote economic growth while keeping disparities within a reasonable limit. I believe this approach best balances fairness and efficiency, which aligns with my values as a student concerned about societal stability.\n",
      "2025-08-11 01:51:21,015 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:21,016 - core.phase2_manager - INFO - Vote proposal text: voting on maximizing the average income with a range constraint to balance economic growth and disparities\n",
      "2025-08-11 01:51:21,016 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:21,018 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:21,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:21,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:21,405 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:21,447 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:21,536 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:21,650 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:21,651 - core.phase2_manager - INFO - Agent_1 response: 'NO' -> Contains YES: False\n",
      "2025-08-11 01:51:21,651 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:21,652 - core.phase2_manager - INFO - Agent_3 response: 'NO' -> Contains YES: False\n",
      "2025-08-11 01:51:21,652 - core.phase2_manager - INFO - All agreements: [False, True, False]\n",
      "2025-08-11 01:51:21,653 - core.phase2_manager - INFO - Unanimous result: False\n",
      "2025-08-11 01:51:21,655 - core.phase2_manager - INFO - Unanimous agreement result: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:21,659 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:22,099 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:22,328 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:22,798 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:23,669 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:24,059 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:24,553 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:25,050 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:25,434 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:25,752 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:26,109 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:26,186 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:26,405 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:26,406 - core.phase2_manager - INFO - Agent: Agent_1\n",
      "2025-08-11 01:51:26,407 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint, setting the range limit at a level that balances fairness and efficiency. I believe this approach can promote both social stability and overall wealth, which aligns with my values. \n",
      "\n",
      "My current preferred principle is the range constraint.\n",
      "2025-08-11 01:51:26,408 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:26,408 - core.phase2_manager - INFO - Vote proposal text: vote on the principle of maximizing the average income with a range constraint at a balanced level\n",
      "2025-08-11 01:51:26,408 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:26,415 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:26,416 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:26,416 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:26,996 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:26,998 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:26,999 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:27,001 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:27,001 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:27,001 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:27,002 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:27,002 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:51:27,002 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:51:27,002 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:27,004 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:27,004 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:27,005 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:27,404 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:27,436 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:27,804 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:28,044 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:28,514 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:29,248 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:29,863 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:30,076 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:30,076 - core.phase2_manager - INFO - Agent: Agent_2\n",
      "2025-08-11 01:51:30,077 - core.phase2_manager - INFO - Statement: I propose we vote on maximizing the average income with a range constraint, with the specific limit set at our previous discussion level. I believe this approach offers a good balance between promoting overall economic growth and maintaining fairness by limiting disparities. If everyone agrees, I think we should proceed with this principle.\n",
      "2025-08-11 01:51:30,077 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:30,078 - core.phase2_manager - INFO - Vote proposal text: Vote on maximizing the average income with a range constraint set at the previous discussion level.\n",
      "2025-08-11 01:51:30,078 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:30,083 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:30,084 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:30,084 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:30,512 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:30,529 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:30,542 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:30,562 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:30,587 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:30,588 - core.phase2_manager - INFO - Agent_1 response: 'NO' -> Contains YES: False\n",
      "2025-08-11 01:51:30,588 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:30,589 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:30,589 - core.phase2_manager - INFO - All agreements: [False, True, True]\n",
      "2025-08-11 01:51:30,589 - core.phase2_manager - INFO - Unanimous result: False\n",
      "2025-08-11 01:51:30,589 - core.phase2_manager - INFO - Unanimous agreement result: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:30,592 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:30,921 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:30,984 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:33,870 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:34,189 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:34,285 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:34,586 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:35,040 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:35,483 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:36,253 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:36,676 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:36,883 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:37,379 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:37,522 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:37,523 - core.phase2_manager - INFO - Agent: Agent_3\n",
      "2025-08-11 01:51:37,524 - core.phase2_manager - INFO - Statement: I think we're all in agreement that the principle of maximizing the average income with a range constraint is a good approach. Since Agent_2 has suggested setting the limit at our previous discussion level, I believe we should clarify and confirm that specific constraint to ensure everyone is on the same page. \n",
      "\n",
      "I propose we vote on the principle of maximizing the average income with a range constraint, with the range limit set at the level we previously discussed. \n",
      "\n",
      "If everyone agrees, Im comfortable moving forward with this.\n",
      "2025-08-11 01:51:37,525 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:37,525 - core.phase2_manager - INFO - Vote proposal text: vote on the principle of maximizing the average income with a range constraint, with the range limit set at the previously discussed level\n",
      "2025-08-11 01:51:37,526 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:37,531 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:37,532 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:37,533 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:38,090 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:38,149 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:38,149 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:38,175 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:38,175 - core.phase2_manager - INFO - Agent_1 response: 'NO' -> Contains YES: False\n",
      "2025-08-11 01:51:38,176 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:38,176 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:38,176 - core.phase2_manager - INFO - All agreements: [False, True, True]\n",
      "2025-08-11 01:51:38,176 - core.phase2_manager - INFO - Unanimous result: False\n",
      "2025-08-11 01:51:38,176 - core.phase2_manager - INFO - Unanimous agreement result: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:38,178 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:38,543 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:40,384 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:40,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:40,829 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:40,896 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:41,445 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:41,564 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:41,882 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:42,052 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:43,594 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:43,995 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:44,129 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:44,130 - core.phase2_manager - INFO - Agent: Agent_1\n",
      "2025-08-11 01:51:44,130 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint, with the specific limit set at the level we previously discussed. If everyone agrees, Im comfortable moving forward with this. \n",
      "\n",
      "I believe this approach best balances economic growth and fairness, which aligns with my values as a student concerned about societal stability.\n",
      "2025-08-11 01:51:44,131 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:44,131 - core.phase2_manager - INFO - Vote proposal text: vote on the principle of maximizing the average income with a range constraint at the predetermined limit\n",
      "2025-08-11 01:51:44,132 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:44,137 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:44,138 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:44,138 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:44,609 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:44,612 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:45,677 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:45,713 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:45,714 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:45,715 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:45,715 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:45,716 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:51:45,716 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:51:45,717 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:45,722 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:45,722 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:45,723 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:46,087 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:46,090 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:46,105 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:46,135 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:46,438 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:46,896 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:47,275 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:47,462 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:47,462 - core.phase2_manager - INFO - Agent: Agent_2\n",
      "2025-08-11 01:51:47,462 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint, setting the range limit at a level that balances fairness and efficiency. I believe this approach can promote both social stability and overall wealth, which aligns with my values. \n",
      "\n",
      "My current preferred principle is the range constraint.\n",
      "2025-08-11 01:51:47,462 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:47,462 - core.phase2_manager - INFO - Vote proposal text: vote on the principle of maximizing the average income with a range constraint, balancing fairness and efficiency\n",
      "2025-08-11 01:51:47,463 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:47,464 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:47,465 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:47,465 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:47,888 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:47,914 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:47,922 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:47,967 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:47,968 - core.phase2_manager - INFO - Agent_1 response: 'NO' -> Contains YES: False\n",
      "2025-08-11 01:51:47,968 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:47,969 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:47,969 - core.phase2_manager - INFO - All agreements: [False, True, True]\n",
      "2025-08-11 01:51:47,969 - core.phase2_manager - INFO - Unanimous result: False\n",
      "2025-08-11 01:51:47,970 - core.phase2_manager - INFO - Unanimous agreement result: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:47,973 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:48,414 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:48,441 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:48,827 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:50,494 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:50,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:51,206 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:51,653 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:52,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:52,738 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:53,163 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:54,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:54,135 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:54,545 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:54,684 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:54,685 - core.phase2_manager - INFO - Agent: Agent_3\n",
      "2025-08-11 01:51:54,685 - core.phase2_manager - INFO - Statement: I think we've all expressed a strong preference for the range constraint principle, but it seems we're not quite there yet in reaching consensus. I propose we take a moment to discuss whether we can agree on a specific range limit that everyone finds acceptable, or if we should consider a different principle altogether. \n",
      "\n",
      "If we can agree on a particular range constraint, I believe we can move forward with a vote. Otherwise, I suggest we consider voting on the principle of maximizing the average income with a range constraint, but with a specific range limit that we all agree on.\n",
      "\n",
      "For now, I still favor the range constraint principle because I believe it offers a good balance between fairness and prosperity. \n",
      "\n",
      "Let's see if we can find common ground on the range limit or decide to vote on that specific version.\n",
      "2025-08-11 01:51:54,686 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:54,686 - core.phase2_manager - INFO - Vote proposal text: vote on the specific range limit or the principle of maximizing average income with a range constraint\n",
      "2025-08-11 01:51:54,687 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:54,692 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:54,693 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:54,693 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:55,155 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:55,157 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:55,219 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:55,359 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:51:55,360 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:55,360 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:55,360 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:51:55,360 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:51:55,361 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:51:55,361 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:55,362 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:55,363 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:55,363 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:55,539 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:55,748 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:55,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:55,802 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:56,048 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:56,243 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:56,706 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:57,035 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:57,432 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:58,652 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:59,055 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:59,190 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:51:59,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:51:59,827 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:51:59,828 - core.phase2_manager - INFO - Agent: Agent_3\n",
      "2025-08-11 01:51:59,828 - core.phase2_manager - INFO - Statement: I think we're all leaning towards the principle of maximizing the average income with a range constraint, but it seems we haven't yet agreed on the specific limit for that range. To move forward, I propose we clarify and agree on the exact range constraint level we discussed earlier. Once we have that, I believe we can reach consensus. \n",
      "\n",
      "So, I propose we vote on adopting the principle of maximizing the average income with a range constraint set at the previously discussed level. If everyone agrees, Im comfortable moving forward with this.\n",
      "2025-08-11 01:51:59,828 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:51:59,828 - core.phase2_manager - INFO - Vote proposal text: voting on adopting the principle of maximizing the average income with a specified range constraint at the previously discussed level\n",
      "2025-08-11 01:51:59,829 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:59,832 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:59,833 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:51:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:51:59,833 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:00,252 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:00,270 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:00,361 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:00,363 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:52:00,364 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:52:00,364 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:52:00,364 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:52:00,364 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:52:00,364 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:52:00,365 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:00,366 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:00,367 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:00,367 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:00,725 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:00,776 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:00,947 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:01,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:01,551 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:02,252 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:02,642 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:02,972 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:03,788 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:03,907 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:04,372 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:05,852 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:05,853 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:06,272 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:06,310 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:06,396 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:06,830 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:08,824 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:09,299 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:10,143 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:10,145 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:10,618 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:10,646 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:11,685 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:11,859 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:12,109 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:13,710 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:14,021 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:14,495 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:14,676 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-11 01:52:14,676 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-11 01:52:14,676 - core.experiment_manager - INFO - Phase 2 completed with consensus on Maximizing the average with a range constraint\n",
      "2025-08-11 01:52:14,677 - core.experiment_manager - INFO - Experiment b6e7087d-d783-4e50-b879-ed0825c8d048 completed successfully in 125.99 seconds\n",
      "2025-08-11 01:52:14,682 - core.experiment_manager - INFO - Results saved to: hypothesis_2_&_4/logs/config_02_results_20250811_015008.json\n",
      "2025-08-11 01:52:14,682 - utils.experiment_runner - INFO - Completed experiment: config_02 -> hypothesis_2_&_4/logs/config_02_results_20250811_015008.json\n",
      "2025-08-11 01:52:14,683 - utils.experiment_runner - INFO - Starting experiment: config_03\n",
      "2025-08-11 01:52:14,686 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-11 01:52:14,687 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:52:14,687 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:52:14,687 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-nano, temp=0.0)\n",
      "2025-08-11 01:52:14,687 - core.experiment_manager - INFO - Starting Phase 1 for experiment 96132cc6-a0ae-4192-8af2-e6d105edfb8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:14,690 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:14,690 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:14,691 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:15,104 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:15,151 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:15,171 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:16,100 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:16,101 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:16,229 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:16,535 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:16,546 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:16,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:16,847 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:16,975 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:17,079 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:52:17,079 - core.phase2_manager - INFO - Agent: Agent_1\n",
      "2025-08-11 01:52:17,080 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint, setting the range limit at the level we previously discussed. If everyone agrees, Im comfortable moving forward with this. \n",
      "\n",
      "I believe this approach best balances economic growth and fairness, which aligns with my values as a student concerned about societal stability.\n",
      "2025-08-11 01:52:17,080 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:52:17,080 - core.phase2_manager - INFO - Vote proposal text: vote on the principle of maximizing the average income with a range constraint at the previously discussed level\n",
      "2025-08-11 01:52:17,080 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:17,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:17,083 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:17,083 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:17,415 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:17,462 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:17,569 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:17,598 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:52:17,598 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:52:17,598 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:52:17,599 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:52:17,599 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:52:17,599 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:52:17,599 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:17,601 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:17,601 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:17,601 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:17,934 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:18,382 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:18,390 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:18,641 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:18,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:19,074 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:19,138 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:19,281 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:19,629 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:19,725 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:20,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:20,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:20,799 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:21,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:21,917 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:22,134 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:22,286 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:22,552 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:23,407 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:24,121 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:24,528 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:24,753 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:25,174 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:25,591 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:26,051 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:26,669 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:27,122 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:27,340 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:27,694 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:28,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:28,498 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:30,051 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:30,077 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:30,436 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:30,482 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:30,576 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:30,894 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:31,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:31,202 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:31,409 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:31,525 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:31,610 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:31,883 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:32,741 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:32,880 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:33,204 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:33,319 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:34,017 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:34,132 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:34,245 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:34,580 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:34,580 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:34,592 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:35,149 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:35,527 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:35,525 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:35,896 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:35,965 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:36,628 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:37,196 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:37,709 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:38,187 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:38,349 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:38,535 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:38,847 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:38,898 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:39,346 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:39,490 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:39,832 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:39,875 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:52:39,875 - core.phase2_manager - INFO - Agent: Agent_2\n",
      "2025-08-11 01:52:39,876 - core.phase2_manager - INFO - Statement: I propose we take a moment to clarify and agree on the specific range constraint level we discussed earlier. Once we have that clear, I believe we can reach consensus on adopting the principle of maximizing the average income with that range constraint. If everyone agrees, Im comfortable moving forward with this. \n",
      "\n",
      "I still believe this approach best balances economic growth and fairness, which aligns with my values as a student concerned about societal stability.\n",
      "2025-08-11 01:52:39,876 - core.phase2_manager - INFO - Vote proposal detected: False\n",
      "2025-08-11 01:52:39,877 - core.phase2_manager - INFO - No vote proposal detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:39,880 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:40,210 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:40,281 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:40,352 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:40,382 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:40,616 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:40,721 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:40,811 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:41,406 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:41,955 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:42,097 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:42,465 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:42,518 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:42,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:42,957 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:43,351 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:43,914 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:44,353 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:44,606 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:44,817 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:45,123 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:45,162 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:45,899 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:46,190 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:46,232 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:46,322 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:46,616 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:46,681 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:47,149 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:47,543 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:47,571 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:52:47,572 - core.phase2_manager - INFO - Agent: Agent_1\n",
      "2025-08-11 01:52:47,572 - core.phase2_manager - INFO - Statement: I propose we take a moment to clarify and agree on the specific range constraint level we discussed earlier. Once we have that clear, I believe we can reach consensus on adopting the principle of maximizing the average income with that range constraint. If everyone agrees, Im comfortable moving forward with this.\n",
      "2025-08-11 01:52:47,572 - core.phase2_manager - INFO - Vote proposal detected: False\n",
      "2025-08-11 01:52:47,572 - core.phase2_manager - INFO - No vote proposal detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:47,573 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:47,865 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:48,034 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:48,630 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:49,056 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:49,430 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:49,467 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:49,951 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:50,170 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:50,173 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:50,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:50,607 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:50,726 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:50,822 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:52,032 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:52,396 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:53,636 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:53,941 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:54,120 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:54,407 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:55,594 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:55,989 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:56,289 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:56,535 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:52:56,568 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:52:56,568 - core.phase2_manager - INFO - Agent: Agent_2\n",
      "2025-08-11 01:52:56,569 - core.phase2_manager - INFO - Statement: I propose we take a moment to clarify and agree on the specific range constraint level we discussed earlier. Once we have that clear, I believe we can reach consensus on adopting the principle of maximizing the average income with that range constraint. If everyone agrees, Im comfortable moving forward with this.\n",
      "2025-08-11 01:52:56,569 - core.phase2_manager - INFO - Vote proposal detected: False\n",
      "2025-08-11 01:52:56,569 - core.phase2_manager - INFO - No vote proposal detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:56,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:52:57,000 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:52:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:52:59,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:00,182 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:00,705 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:00,876 - core.experiment_manager - INFO - Phase 1 completed. 3 participants finished.\n",
      "2025-08-11 01:53:00,876 - core.experiment_manager - INFO - Agent_1: $6.95 earned\n",
      "2025-08-11 01:53:00,876 - core.experiment_manager - INFO - Agent_2: $8.94 earned\n",
      "2025-08-11 01:53:00,877 - core.experiment_manager - INFO - Agent_3: $9.40 earned\n",
      "2025-08-11 01:53:00,877 - core.experiment_manager - INFO - Starting Phase 2 for experiment 96132cc6-a0ae-4192-8af2-e6d105edfb8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:00,878 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:01,205 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:01,310 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:04,016 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:04,340 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:04,528 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:04,761 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:04,786 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:53:04,786 - core.phase2_manager - INFO - Agent: Agent_3\n",
      "2025-08-11 01:53:04,787 - core.phase2_manager - INFO - Statement: I propose we take a moment to clarify and agree on the specific range constraint level we discussed earlier. Once we have that clear, I believe we can reach consensus on adopting the principle of maximizing the average income with that range constraint. If everyone agrees, Im comfortable moving forward with this.\n",
      "2025-08-11 01:53:04,787 - core.phase2_manager - INFO - Vote proposal detected: False\n",
      "2025-08-11 01:53:04,788 - core.phase2_manager - INFO - No vote proposal detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:04,790 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:04,903 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:05,346 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:05,758 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:07,658 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:08,044 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:08,897 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:09,397 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:09,844 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:10,191 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:10,471 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:53:10,472 - core.phase2_manager - INFO - Agent: Agent_2\n",
      "2025-08-11 01:53:10,472 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint. I believe this approach balances fairness and efficiency by increasing overall wealth while limiting income disparities. This seems like a fair compromise that considers both the well-being of the poorest and the overall growth of income. \n",
      "\n",
      "My current preferred principle remains the same: maximizing the average income with a range constraint.\n",
      "2025-08-11 01:53:10,472 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:53:10,472 - core.phase2_manager - INFO - Vote proposal text: Vote on the principle of maximizing the average income with a range constraint\n",
      "2025-08-11 01:53:10,472 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:10,474 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:10,475 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:10,475 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:10,911 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:10,928 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:11,239 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:11,289 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:53:11,290 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:53:11,290 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:53:11,291 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:53:11,291 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:53:11,292 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:53:11,292 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:11,296 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:11,297 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:11,298 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:11,717 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:11,740 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:11,755 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:12,005 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:12,571 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:15,521 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:16,169 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:16,406 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:16,814 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:16,864 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:53:16,864 - core.phase2_manager - INFO - Agent: Agent_2\n",
      "2025-08-11 01:53:16,864 - core.phase2_manager - INFO - Statement: I propose we take a moment to clarify and agree on the specific range constraint level we discussed earlier. Once we have that clear, I believe we can reach consensus on adopting the principle of maximizing the average income with that range constraint. If everyone agrees, Im comfortable moving forward with this.\n",
      "2025-08-11 01:53:16,864 - core.phase2_manager - INFO - Vote proposal detected: False\n",
      "2025-08-11 01:53:16,865 - core.phase2_manager - INFO - No vote proposal detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:16,866 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:17,282 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:20,369 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:20,499 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:20,777 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:20,894 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:21,538 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:21,993 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:22,295 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:23,054 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:23,841 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:24,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:24,261 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:53:24,262 - core.phase2_manager - INFO - Agent: Agent_1\n",
      "2025-08-11 01:53:24,262 - core.phase2_manager - INFO - Statement: I propose we take a moment to clarify and agree on the specific range constraint level we discussed earlier. Once we have that clear, I believe we can reach consensus on adopting the principle of maximizing the average income with that range constraint. If everyone agrees, Im comfortable moving forward with this.\n",
      "2025-08-11 01:53:24,262 - core.phase2_manager - INFO - Vote proposal detected: False\n",
      "2025-08-11 01:53:24,262 - core.phase2_manager - INFO - No vote proposal detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:24,264 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:25,475 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:25,486 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:25,986 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:26,710 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:27,107 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:28,794 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:29,187 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:29,778 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:29,849 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:30,175 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:30,287 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:30,343 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:53:30,344 - core.phase2_manager - INFO - Agent: Agent_1\n",
      "2025-08-11 01:53:30,344 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint. I believe this approach strikes a good balance between increasing overall wealth and maintaining fairness by limiting income disparities. It aligns with my values of fairness and social stability while also promoting efficiency. \n",
      "\n",
      "My current preferred principle remains the same: maximizing the average income with a range constraint.\n",
      "2025-08-11 01:53:30,344 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:53:30,345 - core.phase2_manager - INFO - Vote proposal text: vote on the principle of maximizing the average income with a range constraint\n",
      "2025-08-11 01:53:30,345 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:30,347 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:30,348 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:30,348 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:30,798 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:30,800 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:30,916 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:30,963 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:53:30,963 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:53:30,964 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:53:30,964 - core.phase2_manager - INFO - Agent_3 response: 'NO' -> Contains YES: False\n",
      "2025-08-11 01:53:30,964 - core.phase2_manager - INFO - All agreements: [True, True, False]\n",
      "2025-08-11 01:53:30,965 - core.phase2_manager - INFO - Unanimous result: False\n",
      "2025-08-11 01:53:30,965 - core.phase2_manager - INFO - Unanimous agreement result: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:30,968 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:31,491 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:32,687 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:33,149 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:33,411 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:33,462 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:33,862 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:34,132 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:34,203 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:53:34,203 - core.phase2_manager - INFO - Agent: Agent_3\n",
      "2025-08-11 01:53:34,204 - core.phase2_manager - INFO - Statement: I propose we take a moment to clarify and agree on the specific range constraint level we discussed earlier. Once we have that clear, I believe we can reach consensus on adopting the principle of maximizing the average income with that range constraint. If everyone agrees, Im comfortable moving forward with this.\n",
      "2025-08-11 01:53:34,204 - core.phase2_manager - INFO - Vote proposal detected: False\n",
      "2025-08-11 01:53:34,204 - core.phase2_manager - INFO - No vote proposal detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:34,206 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:34,585 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:35,417 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:35,920 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:35,989 - core.phase2_manager - INFO - === VOTE DETECTION DEBUG ===\n",
      "2025-08-11 01:53:35,990 - core.phase2_manager - INFO - Agent: Agent_3\n",
      "2025-08-11 01:53:35,990 - core.phase2_manager - INFO - Statement: I propose we vote on the principle of maximizing the average income with a range constraint. I believe this approach best balances the goals of increasing overall prosperity while preventing extreme income disparities, which I value highly. \n",
      "\n",
      "My current preferred principle remains the same: maximizing the average income with a range constraint.\n",
      "2025-08-11 01:53:35,991 - core.phase2_manager - INFO - Vote proposal detected: True\n",
      "2025-08-11 01:53:35,991 - core.phase2_manager - INFO - Vote proposal text: vote on the principle of maximizing the average income with a range constraint\n",
      "2025-08-11 01:53:35,992 - core.phase2_manager - INFO - Checking unanimous agreement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:35,996 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:35,996 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:35,997 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:36,458 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:36,509 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:36,524 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:36,524 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:36,618 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:36,654 - core.phase2_manager - INFO - === UNANIMOUS AGREEMENT DEBUG ===\n",
      "2025-08-11 01:53:36,654 - core.phase2_manager - INFO - Agent_1 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:53:36,654 - core.phase2_manager - INFO - Agent_2 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:53:36,654 - core.phase2_manager - INFO - Agent_3 response: 'YES' -> Contains YES: True\n",
      "2025-08-11 01:53:36,655 - core.phase2_manager - INFO - All agreements: [True, True, True]\n",
      "2025-08-11 01:53:36,655 - core.phase2_manager - INFO - Unanimous result: True\n",
      "2025-08-11 01:53:36,655 - core.phase2_manager - INFO - Unanimous agreement result: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:36,657 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:36,657 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:36,658 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:36,943 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:36,943 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:36,979 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:37,084 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:37,108 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:37,506 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:38,035 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:41,283 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:41,653 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:41,998 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:41,999 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:42,470 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 01:53:42,497 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m01:53:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 01:53:43,908 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-nano; provider = openrouter\n",
      "2025-08-11 01:53:44,266 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the experiments\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[43mrun_experiments_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_PARALLEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParallel execution completed at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/utils/experiment_runner.py:338\u001b[39m, in \u001b[36mrun_experiments_parallel\u001b[39m\u001b[34m(config_files, max_parallel, output_dir, verbose)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiments_parallel\u001b[39m(\n\u001b[32m    321\u001b[39m     config_files: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    322\u001b[39m     max_parallel: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    323\u001b[39m     output_dir: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mhypothesis_2_&_4/logs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    324\u001b[39m     verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    325\u001b[39m ) -> List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    Synchronous wrapper for running experiments in parallel.\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m \u001b[33;03m    - List of experiment results dictionaries\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_experiments_parallel_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28mself\u001b[39m._selector.control(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Set up execution parameters\n",
    "MAX_PARALLEL = 2 # Number of experiments to run concurrently\n",
    "OUTPUT_DIR = \"hypothesis_2_&_4/logs\"  # Directory to save results\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Starting parallel execution of {len(config_files)} experiments\")\n",
    "print(f\"Maximum parallel experiments: {MAX_PARALLEL}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run the experiments\n",
    "try:\n",
    "    results = run_experiments_parallel(\n",
    "        config_files=config_files,\n",
    "        max_parallel=MAX_PARALLEL,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Parallel execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nParallel execution failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115231be",
   "metadata": {},
   "source": [
    "# Example: Run Config_01 from Condition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5mhum5el7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "- Language: English\n",
      "- Number of agents: 5\n",
      "- Phase 2 rounds: 20\n",
      "- Utility agent model: gpt-4.1-mini\n",
      "\n",
      "Agent details:\n",
      "  - Agent_1: google/gemini-2.5-pro (temp=0.0)\n",
      "  - Agent_2: google/gemini-2.5-pro (temp=0.0)\n",
      "  - Agent_3: openai/gpt-4.1-mini (temp=0.0)\n",
      "  - Agent_4: anthropic/claude-sonnet-4 (temp=0.0)\n",
      "  - Agent_5: anthropic/claude-sonnet-4 (temp=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Load config_01.yaml\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "config_path = 'hypothesis_2_&_4/configs/condition_1/config_01.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(f\"- Language: {config_dict['language']}\")\n",
    "print(f\"- Number of agents: {len(config_dict['agents'])}\")\n",
    "print(f\"- Phase 2 rounds: {config_dict['phase2_rounds']}\")\n",
    "print(f\"- Utility agent model: {config_dict['utility_agent_model']}\")\n",
    "print(\"\\nAgent details:\")\n",
    "for agent in config_dict['agents']:\n",
    "    print(f\"  - {agent['name']}: {agent['model']} (temp={agent['temperature']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sqn0t3lc8rs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with config_01...\n",
      "Results will be saved to: hypothesis_2_&_4/logs/config_01_results_20250810_103542.json\n",
      "\n",
      "============================================================\n",
      "\n",
      "Experiment failed with error: name 'config_dict' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/wf/_h59fnv53s7476fhw5sn5smh0000gn/T/ipykernel_65546/2338803112.py\", line 12, in <module>\n",
      "    config_dict=config_dict,\n",
      "                ^^^^^^^^^^^\n",
      "NameError: name 'config_dict' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Create output path with timestamp in logs folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = f\"hypothesis_2_&_4/logs/config_01_results_{timestamp}.json\"\n",
    "\n",
    "print(f\"Running experiment with config_01...\")\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Run the experiment\n",
    "try:\n",
    "    results = run_experiment(\n",
    "        config_dict=config_dict,\n",
    "        output_path=output_path,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nExperiment failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41757a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 configuration files:\n",
      "   1. config_01.yaml\n",
      "   2. config_02.yaml\n",
      "   3. config_03.yaml\n",
      "   4. config_04.yaml\n",
      "   5. config_05.yaml\n",
      "   6. config_06.yaml\n",
      "   7. config_07.yaml\n",
      "   8. config_08.yaml\n",
      "   9. config_09.yaml\n",
      "  10. config_10.yaml\n",
      "\n",
      "Total configurations to run: 10\n"
     ]
    }
   ],
   "source": [
    "# Get all config files from condition_1\n",
    "config_dir = \"hypothesis_2_&_4/configs/condition_1\"\n",
    "config_files = sorted(glob.glob(f\"{config_dir}/config_*.yaml\"))\n",
    "\n",
    "print(f\"Found {len(config_files)} configuration files:\")\n",
    "for i, config_file in enumerate(config_files, 1):\n",
    "    config_name = Path(config_file).name\n",
    "    print(f\"  {i:2d}. {config_name}\")\n",
    "\n",
    "# Display the total\n",
    "print(f\"\\nTotal configurations to run: {len(config_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel execution of 10 experiments\n",
      "Maximum parallel experiments: 5\n",
      "Output directory: hypothesis_2_&_4/logs\n",
      "Start time: 2025-08-10 23:21:42\n",
      "\n",
      "================================================================================\n",
      "\n",
      "2025-08-10 23:21:42,235 - utils.experiment_runner - INFO - Loaded 10 configurations\n",
      "2025-08-10 23:21:42,236 - utils.experiment_runner - INFO - Running with max 5 parallel experiments\n",
      "2025-08-10 23:21:42,236 - utils.experiment_runner - INFO - Starting parallel experiment execution...\n",
      "2025-08-10 23:21:42,238 - utils.experiment_runner - INFO - Starting experiment: config_01\n",
      "2025-08-10 23:21:42,250 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,257 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:42,258 - core.experiment_manager - INFO - Created participant: Agent_2 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:42,258 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,258 - core.experiment_manager - INFO - Created participant: Agent_4 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,258 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,263 - core.experiment_manager - INFO - Starting Phase 1 for experiment 637e3be6-986a-452f-916b-5853f5539f4e\n",
      "2025-08-10 23:21:42,264 - utils.experiment_runner - INFO - Starting experiment: config_02\n",
      "2025-08-10 23:21:42,268 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,268 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,269 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 23:21:42,269 - core.experiment_manager - INFO - Created participant: Agent_3 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,269 - core.experiment_manager - INFO - Created participant: Agent_4 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,270 - core.experiment_manager - INFO - Created participant: Agent_5 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,270 - core.experiment_manager - INFO - Starting Phase 1 for experiment 90755503-f1ab-44c8-86e6-89d0480ae5b7\n",
      "2025-08-10 23:21:42,272 - utils.experiment_runner - INFO - Starting experiment: config_03\n",
      "2025-08-10 23:21:42,275 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,276 - core.experiment_manager - INFO - Created participant: Agent_1 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-10 23:21:42,276 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,276 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,277 - core.experiment_manager - INFO - Created participant: Agent_4 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,277 - core.experiment_manager - INFO - Created participant: Agent_5 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 23:21:42,278 - core.experiment_manager - INFO - Starting Phase 1 for experiment 85dfa257-dff6-41ac-9170-0b4d17beec88\n",
      "2025-08-10 23:21:42,278 - utils.experiment_runner - INFO - Starting experiment: config_04\n",
      "2025-08-10 23:21:42,282 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,282 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 23:21:42,282 - core.experiment_manager - INFO - Created participant: Agent_2 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,282 - core.experiment_manager - INFO - Created participant: Agent_3 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,282 - core.experiment_manager - INFO - Created participant: Agent_4 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:42,283 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,283 - core.experiment_manager - INFO - Starting Phase 1 for experiment 2b47e7d1-1bb7-4fd3-abb2-a919c557a913\n",
      "2025-08-10 23:21:42,283 - utils.experiment_runner - INFO - Starting experiment: config_05\n",
      "2025-08-10 23:21:42,289 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,289 - core.experiment_manager - INFO - Created participant: Agent_1 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-10 23:21:42,289 - core.experiment_manager - INFO - Created participant: Agent_2 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,290 - core.experiment_manager - INFO - Created participant: Agent_3 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,290 - core.experiment_manager - INFO - Created participant: Agent_4 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,291 - core.experiment_manager - INFO - Created participant: Agent_5 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,291 - core.experiment_manager - INFO - Starting Phase 1 for experiment ddaf22ba-7382-48bb-8d8e-7faded780afe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,339 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,340 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,344 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,344 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,344 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:21:42,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,346 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,350 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,351 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,360 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,361 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,364 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,366 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,367 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,372 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,374 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-10 23:21:42,376 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-10 23:21:42,709 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,734 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,756 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_04.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:42,757 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,765 - utils.experiment_runner - INFO - Starting experiment: config_06\n",
      "2025-08-10 23:21:42,769 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,769 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,773 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 23:21:42,777 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,777 - core.experiment_manager - INFO - Created participant: Agent_4 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,778 - core.experiment_manager - INFO - Created participant: Agent_5 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,780 - core.experiment_manager - INFO - Starting Phase 1 for experiment 11de5f06-7c1c-4d3f-9173-30228b75476a\n",
      "2025-08-10 23:21:42,797 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_05.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:42,806 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_02.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:42,811 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,827 - utils.experiment_runner - INFO - Starting experiment: config_07\n",
      "2025-08-10 23:21:42,831 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,831 - core.experiment_manager - INFO - Created participant: Agent_1 (x-ai/grok-4, temp=0.0)\n",
      "2025-08-10 23:21:42,832 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,832 - core.experiment_manager - INFO - Created participant: Agent_3 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:42,832 - core.experiment_manager - INFO - Created participant: Agent_4 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,832 - core.experiment_manager - INFO - Created participant: Agent_5 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,833 - core.experiment_manager - INFO - Starting Phase 1 for experiment 0f38958d-e145-48d9-b289-b8957f82f646\n",
      "2025-08-10 23:21:42,833 - utils.experiment_runner - INFO - Starting experiment: config_08\n",
      "2025-08-10 23:21:42,836 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,837 - core.experiment_manager - INFO - Created participant: Agent_1 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,837 - core.experiment_manager - INFO - Created participant: Agent_2 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,837 - core.experiment_manager - INFO - Created participant: Agent_3 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 23:21:42,837 - core.experiment_manager - INFO - Created participant: Agent_4 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,838 - core.experiment_manager - INFO - Created participant: Agent_5 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,838 - core.experiment_manager - INFO - Starting Phase 1 for experiment 809379c1-4f85-49fe-ad06-e4e45cc10db3\n",
      "2025-08-10 23:21:42,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,851 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,851 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,853 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,856 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,857 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,858 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:42,927 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,938 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_06.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:42,939 - utils.experiment_runner - INFO - Starting experiment: config_09\n",
      "2025-08-10 23:21:42,942 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,942 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,942 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 23:21:42,943 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,943 - core.experiment_manager - INFO - Created participant: Agent_4 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,943 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,943 - core.experiment_manager - INFO - Starting Phase 1 for experiment 2c8ad66b-0f40-4840-a677-83279d5a9bfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:42,955 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,060 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:43,069 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_08.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:43,070 - utils.experiment_runner - INFO - Starting experiment: config_10\n",
      "2025-08-10 23:21:43,075 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_2 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_3 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_4 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_5 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:43,078 - core.experiment_manager - INFO - Starting Phase 1 for experiment abee0236-ba1d-4b1d-9c8c-251783b52122\n",
      "2025-08-10 23:21:43,079 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,081 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,081 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:21:43,111 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,195 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,262 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,274 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,305 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,391 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,410 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,436 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,439 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,468 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,478 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,479 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,483 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,484 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,532 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,564 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,626 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,627 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,722 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,730 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,850 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,901 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,939 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,991 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,006 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,034 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,068 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,201 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,263 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,441 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,546 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:45,238 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:45,287 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:45,306 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:45,307 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:45,341 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:45,487 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:45,746 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:45,874 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,026 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:46,089 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,155 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,270 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,305 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:46,397 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,477 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,515 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,528 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:21:46,543 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:46,593 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:46,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,865 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:46,940 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:46,958 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:46,979 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:47,045 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:47,176 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:47,251 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:47,362 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:47,414 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:47,557 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:47,607 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:47,805 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:47,931 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:47,939 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:21:48,090 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:48,143 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:48,437 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,467 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:48,526 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,818 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,874 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,953 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:49,053 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,065 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:49,348 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:49,429 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:49,516 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,586 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:49,599 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,613 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,940 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,992 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:50,098 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:50,189 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,396 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,402 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:50,468 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,608 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,639 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:50,903 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,979 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:50,988 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:51,008 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:51,033 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:51,089 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:51,120 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:51,424 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:51,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:51,587 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:51,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:51,819 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:51,939 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:52,007 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,053 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,240 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,305 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:21:52,313 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:52,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,529 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:52,827 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,938 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,948 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:21:52,983 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:53,286 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,350 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,393 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,456 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,456 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:53,554 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,574 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,741 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,755 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:21:53,880 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,006 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,126 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,170 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,265 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,435 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,560 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:21:54,704 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,713 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,731 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,846 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:54,909 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,918 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,021 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,087 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,261 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,265 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,376 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,637 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,822 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:55,851 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,041 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,054 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,076 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,248 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,407 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:56,453 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,474 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,496 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,618 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,732 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,821 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:56,923 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,939 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:57,157 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:57,216 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,216 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:57,271 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:57,450 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:21:57,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,547 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,600 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,606 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,840 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,397 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,431 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,597 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:21:58,710 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,772 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:21:58,776 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:58,870 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:59,152 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:59,334 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:59,415 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:59,594 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:59,814 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:59,848 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,355 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,532 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:00,536 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,540 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:00,682 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:00,765 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,822 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,823 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:00,850 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,884 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:00,946 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,960 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:01,126 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,170 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,328 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,336 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,354 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,467 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:01,689 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:01,956 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:02,086 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:02,277 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:02,494 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:02,779 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:02,801 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:02,893 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,224 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,563 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,586 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:03,586 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:03,700 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,799 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,825 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:03,828 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,860 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,884 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:03,968 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,000 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,151 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:04,162 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,181 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,253 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,289 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:04,291 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,292 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,387 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,517 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:04,772 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,831 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,860 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,933 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,020 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:05,135 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:05,154 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:05,214 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,313 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,493 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,593 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,641 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,651 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:05,769 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:05,792 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:05,936 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,194 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,269 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,273 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,326 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,439 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,497 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,497 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,517 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:06,593 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,648 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,680 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,731 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,786 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:06,798 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,868 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,921 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,949 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,043 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,091 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,095 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:07,196 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:07,274 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:07,330 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:07,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:07,575 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,817 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,828 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:07,835 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,969 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:08,002 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:08,119 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:08,338 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:08,359 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:08,373 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:08,401 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:08,414 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:08,710 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:08,732 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:08,933 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:09,061 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:09,177 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:09,349 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:09,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:09,372 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:09,727 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:09,815 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:09,904 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,038 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,114 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:10,126 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,169 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,387 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:10,414 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,429 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:10,514 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,714 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:10,797 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:10,837 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,906 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,923 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,929 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:11,008 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:11,210 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:11,257 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:11,428 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:11,487 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:11,736 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:11,902 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:11,909 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:12,113 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,119 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,120 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,216 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:12,320 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,323 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:12,421 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,423 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:12,628 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:12,629 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:12,839 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:12,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,845 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,845 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:12,942 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,044 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,261 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,266 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:13,313 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,319 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,341 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:13,391 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,402 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,538 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:13,612 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,723 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,818 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,860 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,952 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,000 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:14,002 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,075 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,253 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,461 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:14,522 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:14,527 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,630 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:14,651 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:14,654 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:14,676 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:14,850 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,101 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,214 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,268 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,278 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:15,367 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:15,637 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,654 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,666 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,787 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:15,790 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,792 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:15,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,882 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,908 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:16,074 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,175 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,273 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,312 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,337 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:16,348 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,453 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,505 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,514 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,574 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,659 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,716 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:16,792 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,802 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,861 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,925 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,930 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,989 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,421 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:17,588 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,661 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,825 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:17,827 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,839 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,869 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:18,061 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:18,081 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:18,087 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,139 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:18,149 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:18,262 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,277 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,377 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,423 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,431 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,519 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:19,386 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:19,783 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:19,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:19,796 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:19,968 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,147 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:20,149 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:20,166 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:20,248 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,275 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,299 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,323 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:20,443 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,484 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:20,748 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:20,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:20,812 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:20,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,958 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:21,284 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,294 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:21,498 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,551 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,670 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:21,703 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:21,811 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:21,870 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:21,997 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:22,002 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:22,025 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,396 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,560 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,697 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,824 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:22,939 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:22,967 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,078 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:23,130 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:23,238 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:23,239 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:23,385 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,521 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,558 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:23,676 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,900 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:23,964 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,189 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,447 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,453 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:24,471 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:24,549 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,782 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,852 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:24,889 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,956 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,990 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,020 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:25,022 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,070 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,141 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,226 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:25,266 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,275 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,307 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,455 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,493 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,497 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:25,557 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,841 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,895 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,164 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:26,169 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,172 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:26,470 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:26,497 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,636 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,751 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:26,828 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:26,844 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,953 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:27,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,047 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,254 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,289 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:27,318 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,322 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,332 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,434 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:27,513 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,546 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,709 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,710 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,755 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,910 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:28,122 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,166 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,177 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:28,239 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,271 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,445 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:28,671 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:28,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,880 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:28,936 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,365 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:29,395 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:29,699 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,917 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,996 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:30,035 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,167 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,197 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,207 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,411 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,422 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,422 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:30,427 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,478 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:30,562 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,588 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,627 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:30,631 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,761 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,915 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:31,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,148 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:31,175 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,240 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:31,337 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,442 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,764 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:31,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:31,823 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,929 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:32,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:32,059 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:32,119 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,165 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,409 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,421 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,680 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,728 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,030 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,035 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:33,529 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,534 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,649 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,779 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,891 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:33,951 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,051 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:34,068 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:34,159 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,226 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:34,504 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,556 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,590 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,603 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:34,647 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,759 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:34,786 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:34,837 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:34,955 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:35,016 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:35,031 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:35,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:35,329 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:35,370 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:35,374 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:35,724 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:35,880 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:35,892 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:35,963 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,209 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,280 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:36,415 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:36,617 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:36,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,703 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,715 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:37,064 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:37,190 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:37,512 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:37,567 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:38,014 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:38,079 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:38,469 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:38,709 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:38,997 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:39,327 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,373 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:39,420 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,465 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,574 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,705 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:39,869 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,882 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,956 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:39,972 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:40,003 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:40,158 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:40,421 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:40,791 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:40,813 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:40,815 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,077 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,137 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:41,191 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:41,284 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,348 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:41,361 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,451 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,455 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,463 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:41,532 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:41,556 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:41,670 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,682 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:41,734 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:41,808 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:42,009 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:42,064 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,065 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,072 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,295 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:42,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,540 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,698 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:42,775 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:42,835 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:43,305 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:43,600 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:43,702 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:43,838 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:43,958 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,005 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,031 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:44,204 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:44,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,278 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:44,530 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:44,558 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,636 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,815 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:44,871 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:45,080 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,234 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:45,331 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,483 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:45,662 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,731 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,792 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,874 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:46,047 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:46,142 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:46,171 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:46,196 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:46,451 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:46,459 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:46,598 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:46,760 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:46,785 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:46,918 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:47,021 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:47,468 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:47,524 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:47,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:47,986 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:48,145 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:48,349 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:48,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:49,841 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:50,083 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:50,251 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:50,461 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:50,567 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:50,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:50,777 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,104 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,113 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,626 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,827 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,895 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:52,184 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:52,288 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:52,363 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:52,372 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:52,493 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:52,613 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:52,638 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:52,728 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:52,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:53,029 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:53,055 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:53,255 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:53,429 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:53,462 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:53,746 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:53,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:54,005 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:54,029 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:54,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:54,484 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:54,603 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:54,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:54,718 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:55,002 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:55,091 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:55,276 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:55,369 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:55,772 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:56,467 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:56,834 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:56,860 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:57,173 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:57,179 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:57,366 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:58,023 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:58,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:58,300 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:58,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:58,439 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:58,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:59,273 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:59,468 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:59,796 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:59,800 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:59,845 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:59,993 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:59,995 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:59,995 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:00,037 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:00,083 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:00,151 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:00,321 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:00,398 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:00,522 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:00,526 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:00,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:00,933 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:01,433 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:01,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:01,895 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:02,092 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:02,302 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:02,335 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:02,730 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:02,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:03,213 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:03,287 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:03,466 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:03,661 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:03,663 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:04,075 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:04,186 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:04,326 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:04,360 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:05,761 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:05,848 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:05,881 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:06,092 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:06,460 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:06,609 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:06,611 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:06,650 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:06,669 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:07,199 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:07,334 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:07,665 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:07,695 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:07,715 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:07,855 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:08,185 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:08,197 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:08,285 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:09,234 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:09,269 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:11,375 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:11,531 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:11,688 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:11,955 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:12,085 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:12,346 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:12,635 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:13,292 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:13,557 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:13,576 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:13,824 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:13,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:14,106 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:14,180 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:14,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:14,625 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:14,670 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:15,006 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:15,256 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:15,360 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:15,647 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:17,075 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:17,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:17,899 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:18,057 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:18,514 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:18,533 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:19,377 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:19,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:19,487 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:19,528 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:19,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:19,937 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:20,969 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:21,177 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:21,529 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:21,787 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:23,000 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:23,203 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:23,228 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:23,384 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:23:23,500 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:23,576 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:23,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:24,215 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:24,295 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:24,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:24,827 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:24,940 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:25,004 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:25,316 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:25,497 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:26,325 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:26,378 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:26,412 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:27,340 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:28,151 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,078 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,215 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,508 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,680 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:29,835 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:29,881 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,885 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:30,157 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:30,312 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:30,870 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:31,067 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:31,853 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:31,867 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:32,285 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:32,443 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:33,012 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:34,180 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:34,363 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:34,453 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:34,460 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:34,516 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:35,043 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:35,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:35,484 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:35,669 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:35,743 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:35,869 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:36,193 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:36,292 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:36,594 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:36,640 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:37,064 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:37,578 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:38,020 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:38,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:39,038 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:39,279 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:39,433 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:40,469 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:40,878 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:41,965 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:42,585 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:43,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:44,609 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:44,860 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:45,585 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:45,732 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:47,371 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:47,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:47,487 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:47,761 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:48,145 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:48,238 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:49,340 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:49,692 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:49,805 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:50,332 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:50,675 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:51,018 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:51,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:52,095 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:52,289 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:52,619 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:52,969 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:53,431 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:53,599 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:53,959 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:54,146 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:55,018 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:57,074 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:58,955 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:59,370 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:59,607 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:00,364 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:00,491 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:00,620 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:01,482 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:01,882 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:02,809 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:05,014 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:05,279 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:05,400 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:05,698 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,087 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,393 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,412 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,418 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,601 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,767 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,857 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:06,884 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:06,956 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:07,037 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:07,446 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:07,602 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:08,088 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:08,271 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:09,149 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:09,542 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:11,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:12,268 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:12,366 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:12,526 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:12,597 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:12,936 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:13,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:13,292 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:13,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:13,884 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:14,454 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:14,472 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:14,502 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:14,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:14,889 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:15,016 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:16,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:16,232 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:16,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:16,656 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:16,741 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:16,901 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:17,204 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:17,418 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:17,858 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:17,961 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:17,968 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:19,087 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:19,846 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:20,404 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:20,576 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:20,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:22,071 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:24:22,818 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:24,104 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:24,680 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:25,259 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:25,583 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:26,013 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:26,137 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:26,362 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:26,403 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:26,500 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:26,587 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:26,901 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:27,187 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:27,503 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:27,521 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:27,735 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:27,760 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:28,191 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:28,955 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:29,115 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:29,169 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:29,702 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:29,750 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:30,157 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:30,485 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:30,699 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:32,232 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:34,303 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:34,733 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:35,086 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:35,329 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:35,749 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:35,908 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:35,981 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:36,295 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:37,100 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:37,647 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:38,182 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:38,973 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:40,253 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:40,603 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:40,802 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:41,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:41,708 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:41,826 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:42,096 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:42,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:42,767 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:43,267 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:43,504 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:44,367 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:44,696 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:45,758 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:46,798 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:46,897 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:46,898 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:48,518 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:50,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:50,546 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:50,807 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:51,423 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:52,153 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:53,039 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:53,099 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:53,998 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:57,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:57,776 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:58,435 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:59,029 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:59,260 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:00,453 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:25:01,157 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:01,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:01,316 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:01,729 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:02,131 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:02,486 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:25:02,976 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:03,379 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:03,748 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:04,159 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:05,814 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:06,502 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:06,881 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:07,309 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:08,015 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:08,339 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:08,472 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:08,989 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:08,994 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:10,540 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:25:10,904 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:11,065 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:11,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:11,927 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:12,644 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:12,668 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:14,310 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:15,851 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:16,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:17,364 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:17,445 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:25:17,860 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:19,011 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:19,527 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:20,517 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:20,983 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:23,221 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:23,798 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:25,341 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:25,832 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:26,598 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:28,039 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:29,133 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:29,702 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:30,137 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:30,948 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:31,486 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:31,556 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:32,904 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:33,236 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:33,574 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:34,729 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:35,206 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:36,077 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:36,500 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:38,550 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:38,725 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:39,276 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:40,692 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:40,895 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:43,496 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:45,072 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:46,241 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:46,610 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:47,079 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:47,494 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:48,305 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:48,647 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:48,742 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:49,157 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:50,580 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:52,999 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:57,092 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:57,630 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:58,973 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:59,969 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:00,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:00,593 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:01,154 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:01,665 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:02,632 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:02,653 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:03,235 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:03,326 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:05,533 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:26:05,535 - core.experiment_manager - INFO - Agent_1: $8.61 earned\n",
      "2025-08-10 23:26:05,535 - core.experiment_manager - INFO - Agent_2: $15.49 earned\n",
      "2025-08-10 23:26:05,535 - core.experiment_manager - INFO - Agent_3: $9.65 earned\n",
      "2025-08-10 23:26:05,536 - core.experiment_manager - INFO - Agent_4: $8.34 earned\n",
      "2025-08-10 23:26:05,537 - core.experiment_manager - INFO - Agent_5: $6.04 earned\n",
      "2025-08-10 23:26:05,538 - core.experiment_manager - INFO - Starting Phase 2 for experiment 2c8ad66b-0f40-4840-a677-83279d5a9bfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:05,542 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:26:05,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:06,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:26:08,178 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:09,743 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:09,766 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:09,988 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:26:10,556 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:12,262 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:12,583 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:12,865 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:26:15,336 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:18,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:18,493 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:19,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:20,836 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:20,930 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:21,688 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:21,797 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:22,657 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:23,434 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:23,911 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:24,072 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:25,691 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:26:26,671 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:27,836 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:27,951 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:28,119 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:28,545 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:28,662 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:32,647 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:26:33,449 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:34,298 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:35,069 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:36,781 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:37,477 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:39,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:40,410 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:41,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:41,319 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:41,879 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:41,984 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:42,054 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:42,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:45,665 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:46,799 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:46,997 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:26:47,490 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:26:47,490 - openai._base_client - INFO - Retrying request to /responses in 0.499911 seconds\n",
      "2025-08-10 23:26:49,078 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:50,281 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:51,375 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:52,178 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:52,342 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:26:52,343 - openai._base_client - INFO - Retrying request to /responses in 0.901644 seconds\n",
      "2025-08-10 23:26:52,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:53,163 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:55,693 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:55,949 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:56,753 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:57,689 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:58,400 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:26:58,403 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_2250783194f7aef4c9fd434d14f3019a)\n",
      "2025-08-10 23:26:58,403 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_09.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2025-08-10 23:26:58,928 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:59,839 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:00,871 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:02,768 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:03,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:08,527 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:09,365 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:09,413 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:10,436 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:11,037 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:11,574 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:14,181 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:14,751 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:15,490 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:15,786 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:18,560 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:18,864 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:21,608 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:22,330 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:22,817 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:24,529 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:24,840 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:27:25,227 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:25,673 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:25,960 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:27,708 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:30,102 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:30,238 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:33,101 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:33,722 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:34,859 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:35,431 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:36,507 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:36,910 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:38,895 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:40,488 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:40,987 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:42,707 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:42,965 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:43,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:44,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:45,242 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:45,767 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:51,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:51,375 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:52,301 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:53,135 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:53,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:55,621 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:56,241 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:57,061 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:58,814 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:00,136 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:00,887 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_1: $8.29 earned\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_2: $11.06 earned\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_3: $11.88 earned\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_4: $10.02 earned\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_5: $9.07 earned\n",
      "2025-08-10 23:28:00,890 - core.experiment_manager - INFO - Starting Phase 2 for experiment 85dfa257-dff6-41ac-9170-0b4d17beec88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:00,892 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:28:01,470 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:03,623 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:28:04,156 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:04,745 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:05,303 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:28:05,671 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:07,244 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:15,884 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:28:15,886 - openai._base_client - INFO - Retrying request to /responses in 0.470358 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:17,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:18,497 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:22,082 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:28:22,083 - openai._base_client - INFO - Retrying request to /responses in 0.862530 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:23,865 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:24,368 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:28:24,368 - core.experiment_manager - INFO - Agent_1: $9.25 earned\n",
      "2025-08-10 23:28:24,368 - core.experiment_manager - INFO - Agent_2: $12.24 earned\n",
      "2025-08-10 23:28:24,368 - core.experiment_manager - INFO - Agent_3: $8.19 earned\n",
      "2025-08-10 23:28:24,369 - core.experiment_manager - INFO - Agent_4: $8.77 earned\n",
      "2025-08-10 23:28:24,369 - core.experiment_manager - INFO - Agent_5: $13.91 earned\n",
      "2025-08-10 23:28:24,369 - core.experiment_manager - INFO - Starting Phase 2 for experiment 637e3be6-986a-452f-916b-5853f5539f4e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:24,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:28:25,834 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:26,615 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:29,062 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:28:29,063 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_64fbace3aed44361deff328c51e7f332)\n",
      "2025-08-10 23:28:29,064 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_03.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:32,410 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:34,516 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:39,291 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:40,205 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:28:40,205 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:43,042 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:45,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:46,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:48,388 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:50,992 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:53,628 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:54,340 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:28:54,486 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:56,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:00,353 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:29:01,468 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:29:15,682 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:29:15,684 - openai._base_client - INFO - Retrying request to /responses in 0.448560 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:17,913 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:18,877 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:29:18,979 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:29:21,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:29:21,508 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:29:21,509 - openai._base_client - INFO - Retrying request to /responses in 0.914989 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:24,354 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:29:24,882 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:29:28,371 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:29:28,375 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_d9ca697a8b37d6d874278642f9d1cc76)\n",
      "2025-08-10 23:29:28,377 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_01.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:41,114 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:29:42,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:44,288 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:29:45,273 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:05,695 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:30:06,884 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:09,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:30:10,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:30:30,726 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:30:30,728 - core.experiment_manager - INFO - Agent_1: $11.50 earned\n",
      "2025-08-10 23:30:30,728 - core.experiment_manager - INFO - Agent_2: $9.15 earned\n",
      "2025-08-10 23:30:30,728 - core.experiment_manager - INFO - Agent_3: $9.67 earned\n",
      "2025-08-10 23:30:30,728 - core.experiment_manager - INFO - Agent_4: $11.90 earned\n",
      "2025-08-10 23:30:30,729 - core.experiment_manager - INFO - Agent_5: $14.74 earned\n",
      "2025-08-10 23:30:30,729 - core.experiment_manager - INFO - Starting Phase 2 for experiment abee0236-ba1d-4b1d-9c8c-251783b52122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:30,731 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:30:31,328 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:33,510 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:30:33,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:35,704 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:30:36,417 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:30:52,929 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:30:52,931 - openai._base_client - INFO - Retrying request to /responses in 0.441877 seconds\n",
      "2025-08-10 23:30:58,694 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:30:58,695 - openai._base_client - INFO - Retrying request to /responses in 0.995530 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:59,861 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:31:00,844 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:31:05,554 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:31:05,560 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_c76bb928cef7f413b1223af044ecc1c8)\n",
      "2025-08-10 23:31:05,561 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_10.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:31:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:31:44,235 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:31:45,349 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:33:30,875 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:33:30,878 - core.experiment_manager - INFO - Agent_1: $9.49 earned\n",
      "2025-08-10 23:33:30,878 - core.experiment_manager - INFO - Agent_2: $8.35 earned\n",
      "2025-08-10 23:33:30,879 - core.experiment_manager - INFO - Agent_3: $11.52 earned\n",
      "2025-08-10 23:33:30,880 - core.experiment_manager - INFO - Agent_4: $13.69 earned\n",
      "2025-08-10 23:33:30,880 - core.experiment_manager - INFO - Agent_5: $13.00 earned\n",
      "2025-08-10 23:33:30,881 - core.experiment_manager - INFO - Starting Phase 2 for experiment 0f38958d-e145-48d9-b289-b8957f82f646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:33:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:33:30,886 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:33:33,039 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:33:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:33:48,943 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:33:51,543 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:34:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:34:04,197 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:34:07,320 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:34:27,969 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:34:27,970 - openai._base_client - INFO - Retrying request to /responses in 0.450403 seconds\n",
      "2025-08-10 23:34:33,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:34:33,529 - openai._base_client - INFO - Retrying request to /responses in 0.883945 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the experiments\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[43mrun_experiments_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_PARALLEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParallel execution completed at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/utils/experiment_runner.py:337\u001b[39m, in \u001b[36mrun_experiments_parallel\u001b[39m\u001b[34m(config_files, max_parallel, output_dir, verbose)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiments_parallel\u001b[39m(\n\u001b[32m    320\u001b[39m     config_files: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    321\u001b[39m     max_parallel: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    322\u001b[39m     output_dir: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mhypothesis_2_&_4/logs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    323\u001b[39m     verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    324\u001b[39m ) -> List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    325\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    Synchronous wrapper for running experiments in parallel.\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    335\u001b[39m \u001b[33;03m    - List of experiment results dictionaries\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_experiments_parallel_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28mself\u001b[39m._selector.control(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Set up execution parameters\n",
    "MAX_PARALLEL = 1  # Number of experiments to run concurrently\n",
    "OUTPUT_DIR = \"hypothesis_2_&_4/logs\"  # Directory to save results\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Starting parallel execution of {len(config_files)} experiments\")\n",
    "print(f\"Maximum parallel experiments: {MAX_PARALLEL}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run the experiments\n",
    "try:\n",
    "    results = run_experiments_parallel(\n",
    "        config_files=config_files,\n",
    "        max_parallel=MAX_PARALLEL,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Parallel execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nParallel execution failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    results = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
