{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d36bcd1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6623a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Change to project root directory\n",
    "project_root = '/Users/lucasmuller/Desktop/Githubg/Rawls_v3'\n",
    "os.chdir(project_root)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils.experiment_runner import generate_random_config, generate_and_save_configs, run_experiment, run_experiments_parallel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d1ece",
   "metadata": {},
   "source": [
    "# Parameter Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d50d92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"x-ai/grok-4\",\n",
    "    \"google/gemini-2.5-flash\",\n",
    "    \"google/gemini-2.5-pro\",\n",
    "    \"meta/llama/llama-4-maverick\",\n",
    "    \"anthropic/claude-sonnet-4\",\n",
    "    \"openai/gpt-4.1-mini\",\n",
    "    \"openai/gpt-4.1\",\n",
    "    \"deepseek/deepseek-chat-v3-0324\",\n",
    "    \"qwen/qwen3-coder\",\n",
    "    \"moonshotai/kimi-k2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06703f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_config(\n",
    "      num_agents: int = 3,\n",
    "      num_rounds: int = 20,\n",
    "      personality: str = \"You are an american college student.\",\n",
    "      models: Union[str, List[str]] = models,\n",
    "      temperature: Union[float, tuple] = (0.0, 1.0),\n",
    "      memory_limit: int = 50000,\n",
    "      reasoning_enabled: bool = True,\n",
    "      utility_model: str = \"gpt-4.1-mini\",\n",
    "      language: str = \"English\",\n",
    "      distribution_range_phase1: List[float] = [0.5, 2.0],\n",
    "      distribution_range_phase2: List[float] = [0.5, 2.0]\n",
    "  ) -> Dict[str, Any]:\n",
    "      \"\"\"\n",
    "      Generate a random configuration for the Frohlich Experiment.\n",
    "      \n",
    "      Parameters:\n",
    "      - num_agents: Number of participant agents\n",
    "      - num_rounds: Number of Phase 2 rounds\n",
    "      - personality: Single personality description used for all agents\n",
    "      - models: Single model string or list to randomly select from\n",
    "      - temperature: Fixed temperature (float) or range (tuple) for random selection\n",
    "      - memory_limit: Memory character limit for agents\n",
    "      - reasoning_enabled: Whether agents use reasoning\n",
    "      - utility_model: Model for utility agent\n",
    "      - language: Language for prompts (\"English\", \"Spanish\", \"Mandarin\")\n",
    "      - distribution_range_phase1/2: Income distribution multiplier ranges\n",
    "      \n",
    "      Returns:\n",
    "      - Dictionary representing the experiment configuration\n",
    "      \"\"\"\n",
    "\n",
    "      # Generate agents\n",
    "      agents = []\n",
    "      for i in range(num_agents):\n",
    "          # Generate agent name as Agent_1, Agent_2, etc.\n",
    "          agent_name = f\"Agent_{i + 1}\"\n",
    "\n",
    "          # Select model\n",
    "          if isinstance(models, list):\n",
    "              selected_model = random.choice(models)\n",
    "          else:\n",
    "              selected_model = models\n",
    "\n",
    "          # Select temperature\n",
    "          if isinstance(temperature, tuple) and len(temperature) == 2:\n",
    "              selected_temp = round(random.uniform(temperature[0], temperature[1]), 2)\n",
    "          else:\n",
    "              selected_temp = float(temperature)\n",
    "\n",
    "          agent_config = {\n",
    "              \"name\": agent_name,\n",
    "              \"personality\": personality,\n",
    "              \"model\": selected_model,\n",
    "              \"temperature\": selected_temp,\n",
    "              \"memory_character_limit\": memory_limit,\n",
    "              \"reasoning_enabled\": reasoning_enabled\n",
    "          }\n",
    "          agents.append(agent_config)\n",
    "\n",
    "      # Build complete configuration\n",
    "      config = {\n",
    "          \"language\": language,\n",
    "          \"agents\": agents,\n",
    "          \"utility_agent_model\": utility_model,\n",
    "          \"phase2_rounds\": num_rounds,\n",
    "          \"distribution_range_phase1\": distribution_range_phase1,\n",
    "          \"distribution_range_phase2\": distribution_range_phase2\n",
    "      }\n",
    "\n",
    "      return config\n",
    "\n",
    "def generate_and_save_configs(\n",
    "      num_configs: int = 10,\n",
    "      save_path: str = \"hypothesis_2_&_4/configs/condition_1\",\n",
    "      **config_params\n",
    "  ):\n",
    "      \"\"\"\n",
    "      Generate multiple random configurations and save them as YAML files.\n",
    "      \n",
    "      Parameters:\n",
    "      - num_configs: Number of configurations to generate\n",
    "      - save_path: Path to save the configuration files\n",
    "      - **config_params: Parameters to pass to generate_random_config()\n",
    "      \n",
    "      Returns:\n",
    "      - List of generated configurations\n",
    "      \"\"\"\n",
    "\n",
    "      # Create directory if it doesn't exist\n",
    "      os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "      configs = []\n",
    "      for i in range(num_configs):\n",
    "          # Generate random config\n",
    "          config = generate_random_config(**config_params)\n",
    "          configs.append(config)\n",
    "\n",
    "          # Save to YAML file\n",
    "          filename = f\"config_{i+1:02d}.yaml\"\n",
    "          filepath = os.path.join(save_path, filename)\n",
    "\n",
    "          with open(filepath, 'w') as f:\n",
    "              yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "          print(f\"Saved {filename}\")\n",
    "\n",
    "      print(f\"Generated and saved {num_configs} configurations in {save_path}\")\n",
    "      return configs\n",
    "\n",
    "  # Example usage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe746ee",
   "metadata": {},
   "source": [
    "# 1. Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c5e4f",
   "metadata": {},
   "source": [
    "## Generating Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved config_01.yaml\n",
      "Saved config_02.yaml\n",
      "Saved config_03.yaml\n",
      "Saved config_04.yaml\n",
      "Saved config_05.yaml\n",
      "Saved config_06.yaml\n",
      "Saved config_07.yaml\n",
      "Saved config_08.yaml\n",
      "Saved config_09.yaml\n",
      "Saved config_10.yaml\n",
      "Generated and saved 10 configurations in hypothesis_2_&_4/configs/condition_1\n"
     ]
    }
   ],
   "source": [
    "# Generate configs 1. \n",
    "# Commenteed out to avoid running if accidentally clicked run all \n",
    "#configs = generate_and_save_configs(\n",
    "    num_configs=10,\n",
    "    num_agents=5,\n",
    "    personality=\"You are an american college student.\",\n",
    "    models=models,\n",
    "    temperature=(0),\n",
    "    num_rounds=20,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ba09a",
   "metadata": {},
   "source": [
    "## Running Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all config files from condition_1\n",
    "config_dir = \"hypothesis_2_&_4/configs/condition_1\"\n",
    "config_files = sorted(glob.glob(f\"{config_dir}/config_*.yaml\"))\n",
    "\n",
    "print(f\"Found {len(config_files)} configuration files:\")\n",
    "for i, config_file in enumerate(config_files, 1):\n",
    "    config_name = Path(config_file).name\n",
    "    print(f\"  {i:2d}. {config_name}\")\n",
    "\n",
    "# Display the total\n",
    "print(f\"\\nTotal configurations to run: {len(config_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up execution parameters\n",
    "MAX_PARALLEL = 5  # Number of experiments to run concurrently\n",
    "OUTPUT_DIR = \"hypothesis_2_&_4/logs\"  # Directory to save results\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Starting parallel execution of {len(config_files)} experiments\")\n",
    "print(f\"Maximum parallel experiments: {MAX_PARALLEL}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run the experiments\n",
    "try:\n",
    "    results = run_experiments_parallel(\n",
    "        config_files=config_files,\n",
    "        max_parallel=MAX_PARALLEL,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Parallel execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nParallel execution failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115231be",
   "metadata": {},
   "source": [
    "# Example: Run Config_01 from Condition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5mhum5el7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "- Language: English\n",
      "- Number of agents: 5\n",
      "- Phase 2 rounds: 20\n",
      "- Utility agent model: gpt-4.1-mini\n",
      "\n",
      "Agent details:\n",
      "  - Agent_1: google/gemini-2.5-pro (temp=0.0)\n",
      "  - Agent_2: google/gemini-2.5-pro (temp=0.0)\n",
      "  - Agent_3: openai/gpt-4.1-mini (temp=0.0)\n",
      "  - Agent_4: anthropic/claude-sonnet-4 (temp=0.0)\n",
      "  - Agent_5: anthropic/claude-sonnet-4 (temp=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Load config_01.yaml\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "config_path = 'hypothesis_2_&_4/configs/condition_1/config_01.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(f\"- Language: {config_dict['language']}\")\n",
    "print(f\"- Number of agents: {len(config_dict['agents'])}\")\n",
    "print(f\"- Phase 2 rounds: {config_dict['phase2_rounds']}\")\n",
    "print(f\"- Utility agent model: {config_dict['utility_agent_model']}\")\n",
    "print(\"\\nAgent details:\")\n",
    "for agent in config_dict['agents']:\n",
    "    print(f\"  - {agent['name']}: {agent['model']} (temp={agent['temperature']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sqn0t3lc8rs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with config_01...\n",
      "Results will be saved to: hypothesis_2_&_4/logs/config_01_results_20250810_103542.json\n",
      "\n",
      "============================================================\n",
      "\n",
      "Experiment failed with error: name 'config_dict' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/wf/_h59fnv53s7476fhw5sn5smh0000gn/T/ipykernel_65546/2338803112.py\", line 12, in <module>\n",
      "    config_dict=config_dict,\n",
      "                ^^^^^^^^^^^\n",
      "NameError: name 'config_dict' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Create output path with timestamp in logs folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = f\"hypothesis_2_&_4/logs/config_01_results_{timestamp}.json\"\n",
    "\n",
    "print(f\"Running experiment with config_01...\")\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Run the experiment\n",
    "try:\n",
    "    results = run_experiment(\n",
    "        config_dict=config_dict,\n",
    "        output_path=output_path,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nExperiment failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41757a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 configuration files:\n",
      "   1. config_01.yaml\n",
      "   2. config_02.yaml\n",
      "   3. config_03.yaml\n",
      "   4. config_04.yaml\n",
      "   5. config_05.yaml\n",
      "   6. config_06.yaml\n",
      "   7. config_07.yaml\n",
      "   8. config_08.yaml\n",
      "   9. config_09.yaml\n",
      "  10. config_10.yaml\n",
      "\n",
      "Total configurations to run: 10\n"
     ]
    }
   ],
   "source": [
    "# Get all config files from condition_1\n",
    "config_dir = \"hypothesis_2_&_4/configs/condition_1\"\n",
    "config_files = sorted(glob.glob(f\"{config_dir}/config_*.yaml\"))\n",
    "\n",
    "print(f\"Found {len(config_files)} configuration files:\")\n",
    "for i, config_file in enumerate(config_files, 1):\n",
    "    config_name = Path(config_file).name\n",
    "    print(f\"  {i:2d}. {config_name}\")\n",
    "\n",
    "# Display the total\n",
    "print(f\"\\nTotal configurations to run: {len(config_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel execution of 10 experiments\n",
      "Maximum parallel experiments: 5\n",
      "Output directory: hypothesis_2_&_4/logs\n",
      "Start time: 2025-08-10 10:36:18\n",
      "\n",
      "================================================================================\n",
      "\n",
      "2025-08-10 10:36:18,695 - utils.experiment_runner - INFO - Loaded 10 configurations\n",
      "2025-08-10 10:36:18,695 - utils.experiment_runner - INFO - Running with max 5 parallel experiments\n",
      "2025-08-10 10:36:18,696 - utils.experiment_runner - INFO - Starting parallel experiment execution...\n",
      "2025-08-10 10:36:18,696 - utils.experiment_runner - INFO - Starting experiment: config_01\n",
      "2025-08-10 10:36:18,702 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:18,704 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 10:36:18,704 - core.experiment_manager - INFO - Created participant: Agent_2 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 10:36:18,705 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 10:36:18,705 - core.experiment_manager - INFO - Created participant: Agent_4 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 10:36:18,705 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 10:36:18,706 - utils.language_manager - INFO - Loaded translations for English\n",
      "2025-08-10 10:36:18,709 - core.experiment_manager - INFO - Starting Phase 1 for experiment 0cb973e7-4c30-43e4-8b28-85b05f67c246\n",
      "2025-08-10 10:36:18,710 - utils.experiment_runner - INFO - Starting experiment: config_02\n",
      "2025-08-10 10:36:18,716 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:18,716 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 10:36:18,716 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 10:36:18,716 - core.experiment_manager - INFO - Created participant: Agent_3 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 10:36:18,717 - core.experiment_manager - INFO - Created participant: Agent_4 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 10:36:18,717 - core.experiment_manager - INFO - Created participant: Agent_5 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 10:36:18,717 - core.experiment_manager - INFO - Starting Phase 1 for experiment 91cdce7f-5cc4-459d-8a2e-e82535af2975\n",
      "2025-08-10 10:36:18,717 - utils.experiment_runner - INFO - Starting experiment: config_03\n",
      "2025-08-10 10:36:18,721 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:18,721 - core.experiment_manager - INFO - Created participant: Agent_1 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-10 10:36:18,722 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 10:36:18,722 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 10:36:18,723 - core.experiment_manager - INFO - Created participant: Agent_4 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 10:36:18,723 - core.experiment_manager - INFO - Created participant: Agent_5 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 10:36:18,723 - core.experiment_manager - INFO - Starting Phase 1 for experiment 6f67d1d0-2fa2-4518-a81c-778a52a7f969\n",
      "2025-08-10 10:36:18,724 - utils.experiment_runner - INFO - Starting experiment: config_04\n",
      "2025-08-10 10:36:18,728 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:18,728 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 10:36:18,728 - core.experiment_manager - INFO - Created participant: Agent_2 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 10:36:18,728 - core.experiment_manager - INFO - Created participant: Agent_3 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 10:36:18,729 - core.experiment_manager - INFO - Created participant: Agent_4 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 10:36:18,729 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 10:36:18,729 - core.experiment_manager - INFO - Starting Phase 1 for experiment 93736bc6-46ed-4369-a1d2-3ca95220e440\n",
      "2025-08-10 10:36:18,730 - utils.experiment_runner - INFO - Starting experiment: config_05\n",
      "2025-08-10 10:36:18,733 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:18,733 - core.experiment_manager - INFO - Created participant: Agent_1 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-10 10:36:18,734 - core.experiment_manager - INFO - Created participant: Agent_2 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 10:36:18,734 - core.experiment_manager - INFO - Created participant: Agent_3 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 10:36:18,734 - core.experiment_manager - INFO - Created participant: Agent_4 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 10:36:18,734 - core.experiment_manager - INFO - Created participant: Agent_5 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 10:36:18,734 - core.experiment_manager - INFO - Starting Phase 1 for experiment 556a8682-d501-4134-b3ab-e2366e265e9d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,751 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,752 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,753 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,753 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,754 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,759 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,759 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,760 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:18,760 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,760 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,761 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,761 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,762 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,764 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,766 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,768 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,770 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,770 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,772 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,774 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,774 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,776 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n",
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,776 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:18,782 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-10 10:36:18,786 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-10 10:36:19,136 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 10:36:19,162 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_05.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 10:36:19,163 - utils.experiment_runner - INFO - Starting experiment: config_06\n",
      "2025-08-10 10:36:19,166 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:19,166 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 10:36:19,167 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 10:36:19,167 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 10:36:19,167 - core.experiment_manager - INFO - Created participant: Agent_4 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 10:36:19,167 - core.experiment_manager - INFO - Created participant: Agent_5 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 10:36:19,168 - core.experiment_manager - INFO - Starting Phase 1 for experiment cd2ee2b2-e67c-48a7-a2bb-a36812287219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,170 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,170 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,170 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,171 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,171 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:19,193 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 10:36:19,201 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_04.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 10:36:19,201 - utils.experiment_runner - INFO - Starting experiment: config_07\n",
      "2025-08-10 10:36:19,204 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:19,205 - core.experiment_manager - INFO - Created participant: Agent_1 (x-ai/grok-4, temp=0.0)\n",
      "2025-08-10 10:36:19,205 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 10:36:19,205 - core.experiment_manager - INFO - Created participant: Agent_3 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 10:36:19,205 - core.experiment_manager - INFO - Created participant: Agent_4 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 10:36:19,206 - core.experiment_manager - INFO - Created participant: Agent_5 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 10:36:19,206 - core.experiment_manager - INFO - Starting Phase 1 for experiment 4ca8bc7d-0afa-4b82-911d-da239a1c9059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,217 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,217 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,217 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,217 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,218 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:19,246 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 10:36:19,321 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 10:36:19,333 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 10:36:19,342 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_02.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 10:36:19,342 - utils.experiment_runner - INFO - Starting experiment: config_08\n",
      "2025-08-10 10:36:19,347 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:19,347 - core.experiment_manager - INFO - Created participant: Agent_1 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 10:36:19,347 - core.experiment_manager - INFO - Created participant: Agent_2 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 10:36:19,347 - core.experiment_manager - INFO - Created participant: Agent_3 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 10:36:19,347 - core.experiment_manager - INFO - Created participant: Agent_4 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 10:36:19,348 - core.experiment_manager - INFO - Created participant: Agent_5 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 10:36:19,348 - core.experiment_manager - INFO - Starting Phase 1 for experiment 5afa5818-12d1-44c7-ba17-3f32d5deebda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,350 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,351 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,351 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,351 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,351 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:19,386 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 10:36:19,397 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_06.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 10:36:19,397 - utils.experiment_runner - INFO - Starting experiment: config_09\n",
      "2025-08-10 10:36:19,400 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:19,400 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 10:36:19,401 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 10:36:19,401 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 10:36:19,401 - core.experiment_manager - INFO - Created participant: Agent_4 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 10:36:19,401 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 10:36:19,401 - core.experiment_manager - INFO - Starting Phase 1 for experiment eedc776f-49e5-4258-9ebb-8b712c241786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,405 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,406 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,406 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,406 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,406 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:19,471 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-08-10 10:36:19,500 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 10:36:19,568 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,625 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 10:36:19,634 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_08.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 10:36:19,634 - utils.experiment_runner - INFO - Starting experiment: config_10\n",
      "2025-08-10 10:36:19,638 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:36:19,638 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 10:36:19,639 - core.experiment_manager - INFO - Created participant: Agent_2 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 10:36:19,640 - core.experiment_manager - INFO - Created participant: Agent_3 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 10:36:19,640 - core.experiment_manager - INFO - Created participant: Agent_4 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 10:36:19,640 - core.experiment_manager - INFO - Created participant: Agent_5 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 10:36:19,641 - core.experiment_manager - INFO - Starting Phase 1 for experiment 0db45c62-6d1d-42a3-8373-254bb33feff8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,644 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:19,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:19,719 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,731 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,747 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,748 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,752 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,758 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,772 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,792 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,810 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,811 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,812 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,868 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,902 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:19,929 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,049 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,059 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,087 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,128 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,129 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,130 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,133 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,154 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,167 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,219 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,278 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,424 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,459 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,482 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:20,816 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:21,014 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:21,051 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:21,067 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:21,140 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:21,204 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:21,370 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:21,413 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:21,971 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:22,238 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:22,280 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:22,651 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:22,791 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:22,816 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:22,904 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,050 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:23,128 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,168 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,286 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,319 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:23,466 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,469 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,487 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,488 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:23,517 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,520 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,546 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,673 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,692 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:23,754 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:23,791 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:23,792 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:23,865 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:23,995 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:24,003 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:24,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:24,111 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:24,185 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:24,274 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:24,534 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:24,551 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:24,675 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:24,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:24,771 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:24,968 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:25,008 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:25,152 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:25,292 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:25,315 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:25,353 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:25,412 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:25,492 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:25,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:25,722 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:25,747 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:25,775 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:25,878 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:25,890 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:25,909 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:25,915 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:26,190 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,349 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,440 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:26,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:26,597 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,599 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,600 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,643 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:26,675 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,808 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:26,837 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:26,872 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,917 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,935 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:36:26,978 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:26,985 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:27,007 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:27,257 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:27,340 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:27,428 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:27,686 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:27,708 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:27,897 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:28,015 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:28,067 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:28,358 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:28,421 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:28,646 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:28,732 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:28,840 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:28,884 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:29,278 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:29,402 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:29,405 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:29,608 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:29,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:29,755 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:29,913 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:29,962 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:30,096 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,225 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:30,254 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:30,264 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,285 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:30,295 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,386 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,394 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,423 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,430 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,512 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,529 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,552 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,577 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:30,619 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,677 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,805 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,858 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:30,910 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:30,920 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:30,983 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:31,208 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,247 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,263 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,324 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,371 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:31,380 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:31,418 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:31,434 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:31,503 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:31,573 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:31,677 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:31,770 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:31,773 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,847 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,934 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:31,964 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:31,984 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,989 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,993 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:31,994 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:32,242 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:32,306 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:32,575 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:32,641 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:32,689 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:33,130 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:33,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:33,394 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:33,420 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:33,677 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:33,793 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:33,838 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:34,009 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:34,039 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:34,243 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:34,342 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:34,373 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:34,592 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:34,643 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:34,710 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:34,819 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:34,945 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:34,994 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:35,265 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:35,348 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:35,462 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:35,483 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:35,506 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:35,897 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:36,112 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:36,317 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:36,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:36,511 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:36,515 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:36,604 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:36,729 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:36,861 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:37,033 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:37,038 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:37,071 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:37,136 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:37,361 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:37,365 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:36:37,426 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:37,447 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:36:37,558 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:37,562 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:37,648 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:37,705 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:37,966 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:38,092 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:38,097 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:38,098 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:38,113 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:38,305 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:38,451 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:38,618 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:38,625 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:38,699 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:38,815 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:38,830 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:38,940 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:38,940 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:39,138 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:39,177 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:39,317 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:39,458 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:39,655 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:39,666 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:39,667 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:39,684 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:39,804 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:39,981 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,183 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:40,205 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,320 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:36:40,382 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,518 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,519 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:40,519 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,530 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,538 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,709 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,711 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:40,728 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:40,773 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:40,870 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:40,889 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,033 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:41,056 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:41,086 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,229 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,230 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,230 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,278 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:41,303 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:41,328 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:41,333 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:36:41,374 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:41,376 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,385 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,390 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,454 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:41,464 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:41,656 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:41,787 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:41,925 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:42,081 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:42,081 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:42,090 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:42,102 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:42,278 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:42,330 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:42,445 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:42,873 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:42,880 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:42,886 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:43,154 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:43,367 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:43,423 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:43,499 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:43,550 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:43,553 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:43,654 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:43,851 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:43,852 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:43,895 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:44,020 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:44,041 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:44,042 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:44,059 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:44,253 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:44,263 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:44,412 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:44,449 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:44,474 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:44,715 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:44,717 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:44,779 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:44,896 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:45,018 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:45,425 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:45,434 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:45,446 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:45,565 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:45,612 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:45,773 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:45,928 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:45,945 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:45,965 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:46,161 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:46,279 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:46,332 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:46,546 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:46,551 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:46,577 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:46,646 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:46,801 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:46,804 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:46,967 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:47,126 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:47,128 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:47,138 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:47,363 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:47,454 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:47,583 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:47,629 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:47,688 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:36:47,739 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:47,850 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:48,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:48,189 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:48,214 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:48,321 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:48,413 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:48,494 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:48,566 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:48,605 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:48,623 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:48,675 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:48,773 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:49,023 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:49,027 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:49,071 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:49,133 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:36:49,151 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:49,229 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:49,300 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:49,325 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:49,602 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:49,694 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:49,697 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:49,901 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:50,047 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:50,115 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:50,188 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:50,276 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:50,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:50,656 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:50,733 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:50,795 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:50,818 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:50,932 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:50,946 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:51,031 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:51,093 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:51,374 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:51,473 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:51,493 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:51,644 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:51,679 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:51,679 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:36:51,902 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:51,952 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,037 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,060 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,102 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:36:52,127 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:52,158 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,205 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,258 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:52,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,327 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:52,339 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,349 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:52,525 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:52,570 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:52,698 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:52,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:52,797 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:52,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,816 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:52,848 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:52,888 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:53,187 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:53,235 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:53,254 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:53,306 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:53,458 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:53,473 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:53,530 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:53,679 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:53,701 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:53,733 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:53,804 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:53,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:53,869 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:53,896 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:54,032 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:54,140 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:54,174 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:54,191 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:54,282 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:54,298 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:54,415 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:54,466 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:54,565 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:54,687 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:54,736 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:54,737 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:54,742 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:54,921 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:55,018 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:55,202 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:55,250 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:55,271 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:55,332 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:36:55,435 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:55,764 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:36:55,804 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:55,814 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:55,877 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:55,971 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:56,209 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:56,309 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:56,310 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:56,430 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:56,439 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:56,589 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:56,720 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:56,751 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:56,908 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:56,961 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:57,050 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:57,081 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:57,302 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:57,325 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:57,485 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:57,489 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:57,559 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:57,755 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:57,899 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:57,952 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:58,029 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:58,166 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:58,230 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:58,358 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:58,379 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:58,513 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:58,523 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:36:58,690 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:58,746 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:36:58,780 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:58,793 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:58,817 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:36:58,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:59,091 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:59,325 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:59,400 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:36:59,444 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:36:59,454 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:59,744 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:36:59,848 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:36:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:36:59,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:00,019 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:00,131 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:00,238 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:00,347 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:00,388 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:00,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:00,408 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:00,474 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:00,576 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:00,584 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:00,591 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:00,634 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:00,777 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:00,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:00,862 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:37:00,901 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:00,964 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:01,107 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:01,164 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:01,190 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:01,360 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:01,430 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:01,808 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:01,966 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:02,026 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:02,033 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:02,072 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:02,224 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:37:02,353 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:02,428 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:02,444 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:02,753 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:02,762 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:37:02,934 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:02,977 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:02,980 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:03,268 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:03,277 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:03,290 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:03,334 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:03,356 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:03,357 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:03,394 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:03,456 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:03,563 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:03,606 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:03,693 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:03,698 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:03,707 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:03,935 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:03,962 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:03,987 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:04,146 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:04,147 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:04,179 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:04,365 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:04,421 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:04,481 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:04,532 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:04,699 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:04,716 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:04,871 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:05,083 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:05,089 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:05,093 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:05,110 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:05,266 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:05,475 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:05,629 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:05,695 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:05,767 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:05,779 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:05,920 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:05,965 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:06,054 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:06,095 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:06,103 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:06,130 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:06,216 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:06,249 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:06,312 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:06,370 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:06,404 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:06,413 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:06,568 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:06,845 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:06,848 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:06,861 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:06,878 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:07,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:07,134 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:07,189 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:07,305 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:07,416 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:07,558 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:07,695 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:07,818 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,122 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:08,207 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,263 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,270 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,294 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,590 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,609 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,687 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:08,769 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:08,785 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,832 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:08,954 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:08,958 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:09,053 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:09,208 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:09,237 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:09,288 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:09,371 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:09,414 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:09,504 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:09,557 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:09,615 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:09,654 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:09,694 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:09,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:09,752 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:09,757 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:09,774 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:09,798 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:09,854 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:09,972 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:10,022 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:10,061 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:10,181 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:10,194 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:10,274 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:10,398 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:10,848 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:11,201 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:11,210 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:11,219 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:11,450 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:11,624 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:11,626 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:11,646 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:11,777 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:11,861 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:12,004 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:12,027 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:12,114 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:37:12,281 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:12,589 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:12,601 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:12,643 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:12,672 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:12,766 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:13,137 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:13,183 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:13,431 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:13,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:13,520 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:13,804 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:13,938 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:14,317 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:14,330 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:14,362 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:14,524 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:14,576 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:14,710 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:14,722 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:37:14,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:14,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:15,412 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:15,810 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:15,886 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:16,037 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:16,066 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:16,109 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:16,154 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:16,183 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:16,213 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:16,294 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:37:16,344 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:16,542 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:16,673 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:16,907 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:16,958 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:16,993 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:17,169 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:37:17,174 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:17,283 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:17,327 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:17,354 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:17,386 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:17,665 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:17,680 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:17,691 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:17,705 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:17,816 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:17,870 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:17,902 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:18,287 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:18,304 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:18,757 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:18,834 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:18,942 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:19,620 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:19,949 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:20,298 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:20,405 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:37:20,412 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:20,530 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:20,691 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:20,878 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:20,881 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:21,076 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:21,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:21,201 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:21,228 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:21,433 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:21,481 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:21,635 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:21,861 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:21,980 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:22,218 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:22,626 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:22,652 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:23,229 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:23,670 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:23,755 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:24,013 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:24,258 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:24,405 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:25,187 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:25,265 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:25,560 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:37:26,006 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:26,018 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:26,060 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:26,150 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:26,353 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:26,357 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:26,450 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:37:26,656 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:26,794 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:26,926 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:26,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:27,087 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:37:27,129 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:27,262 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:27,337 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:27,765 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:27,850 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:27,904 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:28,160 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:28,580 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:28,602 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:28,823 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:29,297 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:29,452 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:29,516 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:29,805 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:29,860 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:29,981 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:30,045 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:30,704 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:30,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:30,858 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:30,890 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:31,001 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:31,089 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:31,155 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:31,340 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:31,346 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:31,764 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:31,854 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:32,156 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:32,189 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:32,243 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:32,298 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:32,346 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:32,779 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:33,052 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:33,379 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:33,424 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:37:33,499 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:33,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:33,840 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:34,151 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:34,188 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:34,716 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:34,952 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:35,183 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:35,690 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:35,725 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:36,038 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:36,422 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:36,643 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:37,306 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:37,403 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:37,453 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:37,581 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:37,971 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:38,087 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:38,385 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:38,576 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:38,763 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:38,879 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:39,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:39,026 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:39,085 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:39,317 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:39,317 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:37:39,662 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:39,700 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:39,745 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:39,873 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:39,997 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:37:40,025 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:40,057 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:40,059 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:40,098 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:40,268 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:40,268 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:40,319 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:40,345 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:40,521 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:40,556 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:40,651 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:40,809 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:41,147 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:41,367 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:41,560 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:37:41,914 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:42,257 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:37:42,299 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:42,591 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:42,805 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:42,930 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:37:43,031 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:43,200 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:43,282 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:43,596 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:43,720 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:43,814 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:43,872 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:43,965 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:44,306 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:45,008 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:45,058 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:45,263 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:45,289 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:45,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:45,589 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:45,592 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:37:46,191 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:46,395 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:46,775 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:46,895 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:47,053 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:47,072 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:47,605 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:47,955 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:48,196 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:48,240 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:48,653 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:48,742 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:48,975 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:49,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:49,475 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:50,670 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:51,611 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:51,637 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:52,512 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:52,737 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:52,903 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:53,002 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:53,200 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:53,323 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:53,601 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:53,878 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:37:53,894 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:54,053 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:37:54,395 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:54,411 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:54,880 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:37:54,902 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:55,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:56,613 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:56,771 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:56,962 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:57,346 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:57,373 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:57,429 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:57,498 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:57,873 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:58,093 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:58,120 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:58,235 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:37:58,427 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:58,473 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:58,887 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:37:58,992 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:59,031 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:59,323 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:59,430 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:37:59,499 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:59,600 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:37:59,643 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:37:59,707 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:37:59,822 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:00,173 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:00,229 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:01,783 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:02,247 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:02,363 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:38:02,490 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:02,847 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:03,995 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:38:04,473 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:04,542 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:04,732 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:38:04,760 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:04,813 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:05,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:05,581 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:05,758 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:05,779 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:38:06,288 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:06,418 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:06,443 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:06,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:06,718 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:07,194 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:07,234 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:07,346 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:38:07,934 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:07,950 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:07,956 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:08,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:09,048 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:10,245 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:38:10,561 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:10,582 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:10,914 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:11,213 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:11,440 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:38:12,010 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:12,073 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:12,492 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:13,235 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:13,286 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:38:13,404 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:13,773 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:13,777 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:13,820 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:13,832 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:14,150 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:14,230 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:38:14,521 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:14,779 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:15,426 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:15,934 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:16,606 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:16,817 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:16,877 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:17,341 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:18,320 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:18,517 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:18,970 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:19,330 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:20,391 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:38:22,492 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:22,662 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:22,684 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:22,751 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:23,072 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:38:23,173 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:23,249 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:24,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:24,784 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:24,866 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:38:25,275 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:25,964 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:26,001 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:26,088 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:26,780 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:27,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:38:27,612 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:28,100 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:28,105 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:28,123 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:28,493 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:28,692 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:29,924 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:30,223 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:30,818 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:31,524 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:32,478 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:32,888 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:38:33,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:33,462 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:33,729 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:33,739 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:33,751 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:33,975 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:38:34,014 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:34,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:34,414 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:34,703 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:35,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:35,855 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:36,455 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:38,088 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:39,475 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:39,874 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:40,079 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:40,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:40,175 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:40,191 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:40,468 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:40,540 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:40,621 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:38:40,779 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:41,211 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:41,222 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:41,480 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:41,605 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:41,810 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:42,228 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:38:42,263 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:42,490 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:42,763 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:42,830 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:43,275 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:43,277 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:43,407 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:43,840 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:43,987 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:44,159 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:44,162 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:44,671 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:45,123 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:45,353 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:45,928 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:46,521 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:46,768 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:46,791 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:47,089 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:38:47,244 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:47,312 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:47,901 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:38:48,560 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:49,100 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:50,024 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:50,825 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:50,849 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:38:51,088 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:51,327 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:52,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:52,324 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:52,593 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:52,632 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:52,709 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:52,797 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:52,816 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:38:52,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:53,076 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:38:53,633 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:53,953 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:54,275 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:54,335 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:54,466 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:38:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:38:56,924 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:38:57,712 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:38:58,145 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:00,324 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:01,187 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:01,290 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:01,573 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:39:01,764 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:02,128 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:39:02,406 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:02,815 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:02,893 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:39:03,126 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:03,743 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:03,902 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:04,019 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:04,059 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:39:04,413 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:04,452 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:04,973 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:05,137 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:06,536 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:07,152 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:07,564 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:39:07,666 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:09,310 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:09,924 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:10,678 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:39:12,336 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:13,393 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:13,395 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:13,837 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:14,089 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:14,308 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:14,476 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:14,791 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:14,856 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:39:14,941 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:14,968 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:15,049 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:15,256 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:15,332 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:39:15,379 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:15,798 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:15,875 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:16,409 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:16,523 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:16,589 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:16,983 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:16,985 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:17,284 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:39:18,670 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:18,736 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:19,097 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:21,509 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:21,876 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:22,677 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:39:22,976 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:23,233 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:23,785 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:24,050 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:39:24,250 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:25,038 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:25,573 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:25,782 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:26,335 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:26,543 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:26,703 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:27,047 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:27,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:27,662 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:27,871 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:28,923 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:28,989 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:39:29,400 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:29,409 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:29,898 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:29,963 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:30,474 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:30,545 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:30,825 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:31,352 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:31,788 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:32,155 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:32,171 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:32,196 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:32,224 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:32,532 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:32,883 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:32,942 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:32,944 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:33,185 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:39:33,187 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:33,482 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:33,624 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:33,654 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:34,076 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:34,474 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:39:34,501 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:35,022 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:35,646 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:39:35,775 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:35,962 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:36,641 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:36,955 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:37,189 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:37,587 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:37,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:38,383 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:39,000 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:39,969 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:39:40,352 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:41,150 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:41,647 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:39:41,702 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:43,183 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:44,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:46,623 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:46,858 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:47,339 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:47,779 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:48,560 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:49,280 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:39:49,825 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:49,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:39:50,268 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:50,908 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:50,964 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:51,268 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:52,096 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:52,473 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:53,170 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:53,549 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:54,900 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:55,557 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:39:56,248 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:56,525 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:56,665 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:56,808 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:39:57,320 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:57,717 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:39:57,776 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:58,242 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:39:58,244 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:58,852 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:39:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:39:59,577 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:00,227 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:00,376 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:00,875 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:00,920 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:40:01,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:01,977 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:01,987 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:02,190 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:02,750 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:02,841 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:03,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:40:03,311 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:40:03,628 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:03,650 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:40:04,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:04,730 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:04,832 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:05,571 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:07,430 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:08,051 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:09,111 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:09,317 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:40:10,218 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:11,376 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:40:12,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:12,932 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:13,372 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:13,886 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:14,180 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:14,295 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:14,679 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:40:14,998 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:15,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:40:17,161 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:17,620 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:17,882 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:18,216 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:18,684 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:18,906 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:19,135 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:19,351 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:40:20,244 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:21,576 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:21,693 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:22,567 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:22,569 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:23,426 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:25,652 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:40:25,767 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:40:26,057 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:26,105 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:40:27,449 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:28,602 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:40:28,804 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:30,696 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:30,924 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:40:31,586 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:31,661 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:40:32,983 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:33,000 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:33,214 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:34,879 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:35,977 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:40:36,486 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:37,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:37,642 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:38,165 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:38,595 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:38,893 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:39,120 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:40:40,138 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:40,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:40,764 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:40,941 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:41,262 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 10:40:41,263 - core.experiment_manager - INFO - Agent_1: $5.87 earned\n",
      "2025-08-10 10:40:41,264 - core.experiment_manager - INFO - Agent_2: $8.63 earned\n",
      "2025-08-10 10:40:41,264 - core.experiment_manager - INFO - Agent_3: $6.87 earned\n",
      "2025-08-10 10:40:41,265 - core.experiment_manager - INFO - Agent_4: $8.22 earned\n",
      "2025-08-10 10:40:41,265 - core.experiment_manager - INFO - Agent_5: $6.05 earned\n",
      "2025-08-10 10:40:41,266 - core.experiment_manager - INFO - Starting Phase 2 for experiment 6f67d1d0-2fa2-4518-a81c-778a52a7f969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:41,269 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:40:41,430 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:41,907 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:42,041 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:42,734 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:42,747 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:40:42,826 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:40:43,092 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:43,204 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:44,027 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:44,349 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:47,554 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:48,382 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:40:48,677 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:40:48,820 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:49,004 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:40:49,094 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:51,095 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:51,767 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:51,780 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:40:52,136 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:52,227 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:53,211 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:40:53,389 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:54,565 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:55,862 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:56,052 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:56,273 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:56,419 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:56,470 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:40:56,802 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:57,098 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:57,199 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:40:57,918 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:40:58,301 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:58,759 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:40:59,642 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:41:00,174 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:00,803 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:00,912 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:41:00,962 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:01,239 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:41:01,327 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:01,330 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:02,684 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:41:03,093 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:03,244 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:03,632 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:41:03,969 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:04,052 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:05,662 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:08,338 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:09,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:09,652 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:10,172 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:10,495 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:10,571 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:11,126 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:16,395 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:16,968 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:16,975 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:41:17,118 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:17,547 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:18,587 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:19,690 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:20,844 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:21,591 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:21,811 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:23,492 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:24,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:24,819 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:41:25,515 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:26,095 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:26,708 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:41:27,091 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:27,153 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:41:27,328 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:27,479 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:27,638 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:27,909 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:28,091 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:28,600 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:29,129 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:29,618 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:41:29,682 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:29,853 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:30,364 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:30,515 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:41:33,161 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:33,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:33,770 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:33,885 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:34,074 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:34,273 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:34,387 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:34,532 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:35,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:36,199 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:41:36,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:36,996 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:37,006 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:41:38,308 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:38,824 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:39,068 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:39,084 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:39,085 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:39,085 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:39,086 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:39,086 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:41:39,527 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:39,546 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:39,715 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:39,740 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:39,916 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:40,085 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:40,195 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:40,472 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:40,604 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:40,726 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:42,518 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:43,055 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:41:45,433 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:45,911 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:41:46,098 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:46,272 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:46,531 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:48,129 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:49,152 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:41:49,594 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:51,088 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:51,245 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:51,905 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:52,159 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:52,707 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:53,706 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:54,342 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:41:54,713 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:41:54,815 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:55,510 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:56,100 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:57,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:41:57,755 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:41:58,039 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:58,081 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:41:58,187 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:58,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:58,556 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:41:59,159 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:41:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:41:59,565 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:42:00,578 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:00,998 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:01,472 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:01,484 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:01,485 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:01,486 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:01,487 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:01,487 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:42:01,726 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:01,834 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:01,963 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:01,982 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:02,488 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:02,567 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:03,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:08,047 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 10:42:08,048 - core.experiment_manager - INFO - Agent_1: $8.15 earned\n",
      "2025-08-10 10:42:08,048 - core.experiment_manager - INFO - Agent_2: $9.37 earned\n",
      "2025-08-10 10:42:08,049 - core.experiment_manager - INFO - Agent_3: $9.45 earned\n",
      "2025-08-10 10:42:08,049 - core.experiment_manager - INFO - Agent_4: $10.04 earned\n",
      "2025-08-10 10:42:08,050 - core.experiment_manager - INFO - Agent_5: $6.54 earned\n",
      "2025-08-10 10:42:08,050 - core.experiment_manager - INFO - Starting Phase 2 for experiment eedc776f-49e5-4258-9ebb-8b712c241786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:08,054 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:08,248 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:42:08,802 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:09,371 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:09,680 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:11,410 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:13,153 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:42:13,683 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:14,050 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:14,267 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:14,579 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:42:14,876 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:15,159 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:15,513 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:16,636 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:20,811 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:21,614 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:22,210 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:22,841 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:42:23,234 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:23,681 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:42:23,867 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:24,235 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:24,717 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:25,622 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:25,802 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:31,251 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:32,729 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:33,627 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:33,674 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:34,802 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:42:35,110 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:35,311 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:35,931 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:36,580 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:36,851 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:37,225 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:41,528 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:42:42,008 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:42,121 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:42,392 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:42:42,676 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:42,944 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:44,925 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:45,015 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:42:45,244 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:46,013 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:42:47,154 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:47,567 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:48,469 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:53,621 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:42:53,932 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:54,180 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:42:56,601 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:58,204 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:42:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:42:58,209 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:42:58,715 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:42:59,773 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:00,366 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:43:00,876 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:01,989 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:43:02,604 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:05,222 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:05,486 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:06,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:06,014 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:43:06,595 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:10,972 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:12,413 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:43:13,306 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:16,448 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:16,498 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:43:16,981 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:17,177 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 10:43:17,178 - core.experiment_manager - INFO - Agent_1: $8.33 earned\n",
      "2025-08-10 10:43:17,178 - core.experiment_manager - INFO - Agent_2: $9.98 earned\n",
      "2025-08-10 10:43:17,178 - core.experiment_manager - INFO - Agent_3: $12.38 earned\n",
      "2025-08-10 10:43:17,179 - core.experiment_manager - INFO - Agent_4: $5.92 earned\n",
      "2025-08-10 10:43:17,180 - core.experiment_manager - INFO - Agent_5: $9.81 earned\n",
      "2025-08-10 10:43:17,180 - core.experiment_manager - INFO - Starting Phase 2 for experiment 0cb973e7-4c30-43e4-8b28-85b05f67c246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:17,188 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:18,676 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:43:18,732 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:19,752 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:21,983 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:23,728 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:23,733 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:43:24,365 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:26,145 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:26,149 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:26,149 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:26,149 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:26,149 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:26,150 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:43:26,513 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:26,528 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:26,843 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:26,882 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:26,949 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:27,638 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:27,918 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:43:27,961 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:28,290 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:28,534 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:29,235 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:43:29,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:30,760 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:32,102 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:43:32,390 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:32,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:33,609 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:43:33,753 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:34,024 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:34,148 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:36,039 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:43:36,649 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:36,863 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:36,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:43:37,647 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:39,510 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:40,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:40,264 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:40,265 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:40,266 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:40,266 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:40,267 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:43:40,709 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:40,819 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:40,948 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:41,421 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:41,976 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:43:42,996 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:43,718 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:44,529 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:43:45,156 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:43:45,765 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:46,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:43:47,096 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:49,589 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:43:50,166 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:50,769 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:43:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:43:52,702 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:43:53,280 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:43:56,196 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:44:02,929 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:02,938 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:03,271 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:44:04,575 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:04,698 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:07,037 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:44:12,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:44:12,716 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:12,725 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:12,725 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:12,726 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:12,727 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:12,727 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:44:13,202 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:13,218 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:13,243 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:13,360 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:14,455 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:17,006 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:44:18,061 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:18,404 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:18,405 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:44:18,642 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:19,238 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:20,601 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:23,170 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:44:23,748 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:23,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:44:24,011 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:27,826 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:44:28,460 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:29,491 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:29,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:31,136 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:44:31,217 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:31,960 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:32,542 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:32,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:32,586 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:32,587 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:32,588 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:32,589 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:44:33,097 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:33,105 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:33,139 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:33,350 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:34,119 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:35,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:44:35,400 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:38,153 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:44:39,970 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:40,898 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:41,244 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:44:43,216 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:46,425 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:44:46,709 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:46,716 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:49,095 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:44:49,137 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:50,227 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:52,081 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:52,505 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:44:53,312 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:54,863 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:54,867 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:54,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:54,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:54,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:54,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:44:55,304 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:55,344 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:55,499 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:55,599 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:55,825 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:44:57,710 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:58,796 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:44:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:44:59,352 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:44:59,420 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:00,845 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:03,318 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:06,247 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:45:06,912 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:08,771 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:10,960 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:11,165 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:11,992 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:12,021 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:45:12,395 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:12,824 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:14,011 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:14,023 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:14,199 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:45:14,546 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:45:14,791 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:14,839 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:45:17,190 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:17,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:17,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:17,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:17,196 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:17,196 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:45:17,737 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:17,769 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:17,832 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:17,905 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:18,161 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:20,436 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:21,279 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:45:22,005 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:25,956 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:29,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:45:31,085 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:31,542 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:33,608 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:45:36,531 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:37,201 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:37,964 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:45:38,773 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:42,636 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:45:45,514 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:45,530 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:45,531 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:45,532 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:45,532 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:45,532 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:45:45,927 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:46,098 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:46,256 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:46,331 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:47,739 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:48,393 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:48,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:45:49,374 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:50,290 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:50,292 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:45:51,763 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 10:45:51,764 - core.experiment_manager - INFO - Agent_1: $5.69 earned\n",
      "2025-08-10 10:45:51,764 - core.experiment_manager - INFO - Agent_2: $10.51 earned\n",
      "2025-08-10 10:45:51,764 - core.experiment_manager - INFO - Agent_3: $9.38 earned\n",
      "2025-08-10 10:45:51,765 - core.experiment_manager - INFO - Agent_4: $8.02 earned\n",
      "2025-08-10 10:45:51,766 - core.experiment_manager - INFO - Agent_5: $8.43 earned\n",
      "2025-08-10 10:45:51,766 - core.experiment_manager - INFO - Starting Phase 2 for experiment 0db45c62-6d1d-42a3-8373-254bb33feff8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:51,768 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:52,003 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:45:52,557 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:52,713 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:53,732 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:53,980 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:45:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:45:54,133 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:45:54,693 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:45:59,440 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:01,999 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:46:03,646 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:04,887 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:46:06,363 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:06,367 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:06,367 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:06,369 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:06,369 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:06,369 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:46:06,772 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:06,772 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:06,926 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:07,001 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:07,221 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:09,621 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:09,622 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:09,622 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:09,623 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:09,623 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:46:10,021 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:10,152 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:10,317 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:10,624 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:10,708 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:10,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:46:11,297 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:15,491 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:46:16,747 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:18,659 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:46:19,428 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:19,525 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:22,195 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:24,504 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:46:25,085 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:27,609 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:46:27,693 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 10:46:27,693 - core.experiment_manager - INFO - Agent_1: $8.64 earned\n",
      "2025-08-10 10:46:27,694 - core.experiment_manager - INFO - Agent_2: $9.80 earned\n",
      "2025-08-10 10:46:27,694 - core.experiment_manager - INFO - Agent_3: $8.14 earned\n",
      "2025-08-10 10:46:27,694 - core.experiment_manager - INFO - Agent_4: $9.89 earned\n",
      "2025-08-10 10:46:27,694 - core.experiment_manager - INFO - Agent_5: $11.24 earned\n",
      "2025-08-10 10:46:27,694 - core.experiment_manager - INFO - Starting Phase 2 for experiment 4ca8bc7d-0afa-4b82-911d-da239a1c9059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:27,696 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:46:30,046 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:30,289 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:46:30,954 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:33,191 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:33,816 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:46:34,400 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:34,832 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:46:37,049 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:37,611 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:46:38,667 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:46:39,072 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:42,799 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:46:44,116 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:46:44,532 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:44,547 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:46:45,927 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:46,911 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:46:49,941 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:58,083 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:46:58,790 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:46:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:46:59,502 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:47:00,604 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:02,320 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:07,857 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:07,933 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:07,942 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:07,944 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "\u001b[92m10:47:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:07,944 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:47:07,945 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:47:08,826 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:08,911 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:47:09,597 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:10,222 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:10,406 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:47:10,618 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:11,084 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:11,304 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:11,732 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:12,109 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:16,890 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:17,256 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:17,259 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:47:17,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:17,719 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:18,114 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:19,387 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:19,389 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:19,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:19,639 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 10:47:20,082 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:20,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:20,282 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:21,942 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:22,133 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:47:22,328 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:22,556 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:22,964 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:22,965 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:47:23,261 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:23,476 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:23,596 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:23,612 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:24,975 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:47:25,603 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:26,273 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:26,300 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:26,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:26,609 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:47:26,736 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:26,912 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:26,953 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:27,364 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:28,306 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:28,309 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:47:28,900 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:29,161 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:47:29,161 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:29,473 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:29,670 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:47:29,961 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:31,917 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:47:32,450 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:32,563 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:32,678 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:34,193 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:34,202 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:34,202 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:34,203 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:34,204 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:34,204 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:47:34,522 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:34,698 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:34,976 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:35,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:47:35,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:35,743 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:36,065 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:36,224 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:36,707 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:36,781 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:37,158 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:38,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:38,944 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:47:39,597 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:40,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:40,555 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:40,561 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:40,562 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:40,562 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:40,562 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:40,563 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:47:40,765 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:40,962 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:41,012 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:41,060 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:41,170 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:41,276 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:42,326 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:42,376 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:45,056 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:45,059 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:45,441 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:45,442 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:47:45,835 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:45,891 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:46,412 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:46,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:47:46,854 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:46,946 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:47,328 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:47:47,468 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:48,573 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:52,252 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:52,257 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:47:52,629 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:52,788 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:54,874 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:47:55,369 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:56,594 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:47:56,977 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:47:58,266 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:58,359 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:47:58,721 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:47:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:47:59,289 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:47:59,943 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:00,721 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:00,850 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:48:01,461 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:02,285 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:03,875 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:05,634 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:05,648 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:48:06,036 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:06,051 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:08,124 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:08,381 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:48:08,401 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:08,409 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:08,410 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:08,411 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:08,411 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:08,411 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:48:08,839 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:09,075 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:09,142 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:09,147 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:09,341 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:09,570 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:09,848 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:09,977 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:10,174 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:10,251 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:10,610 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:12,897 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:14,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:48:14,447 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:15,000 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:15,131 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:48:15,743 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:16,255 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:17,628 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:48:18,181 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:20,405 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:48:20,993 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:21,800 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:23,988 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:24,232 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:48:24,978 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:25,760 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:27,094 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:27,099 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:48:27,099 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:48:27,099 - core.experiment_manager - INFO - Phase 2 completed with consensus on Maximizing the floor\n",
      "2025-08-10 10:48:27,101 - core.experiment_manager - INFO - Experiment 6f67d1d0-2fa2-4518-a81c-778a52a7f969 completed successfully in 728.38 seconds\n",
      "2025-08-10 10:48:27,109 - core.experiment_manager - INFO - Results saved to: hypothesis_2_&_4/logs/config_03_results_20250810_103618.json\n",
      "2025-08-10 10:48:27,110 - utils.experiment_runner - INFO - Completed experiment: config_03 -> hypothesis_2_&_4/logs/config_03_results_20250810_103618.json\n",
      "2025-08-10 10:48:27,337 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:27,612 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:29,385 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,391 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,389 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:48:29,389 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:48:29,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,400 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:29,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:48:29,825 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:29,940 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:29,947 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:30,025 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:30,047 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:30,939 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:31,004 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:31,379 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:31,656 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:32,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:32,134 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:32,139 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:32,873 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:48:33,307 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:33,313 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:33,590 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:33,782 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:34,918 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:48:35,348 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:38,924 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:48:39,597 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:39,720 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:48:40,212 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:40,556 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:41,294 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:42,644 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:48:43,199 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:45,952 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:45,957 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:48:46,454 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:46,687 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:48:46,831 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:47,567 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:47,577 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:47,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:47,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:47,579 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:47,579 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:48:47,989 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:48,026 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:48,140 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:48,247 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:49,266 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:52,185 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:53,224 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:48:53,758 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:57,678 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:57,794 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:48:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:48:58,813 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:48:59,279 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:48:59,673 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:03,314 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:03,328 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:49:03,719 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:49:04,290 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:49:04,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:08,445 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:49:09,988 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:49:10,116 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:15,435 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:49:15,539 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:15,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:15,545 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:15,545 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:15,545 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:15,546 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:49:16,136 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:16,342 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:16,417 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:16,708 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:49:16,730 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:17,478 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:17,862 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:21,025 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:21,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:49:23,022 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:26,643 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:49:27,566 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:27,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:27,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:27,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:27,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:27,572 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:49:27,928 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:28,962 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:29,122 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:49:29,351 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:29,794 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:29,933 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:29,943 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:32,160 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:35,307 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:49:36,966 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:37,940 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:38,829 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:49:39,408 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:40,092 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:49:42,535 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:43,516 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:44,689 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:49:46,564 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:48,133 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:49:48,854 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:49:49,495 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:49:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:49:59,011 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:50:00,271 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:50:01,934 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:08,566 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:08,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:08,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:08,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:08,579 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:08,579 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:50:08,909 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:09,037 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:09,038 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:09,354 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:10,274 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:10,807 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:10,810 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:10,811 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:10,811 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:10,811 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:10,811 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:50:10,996 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:50:11,305 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:50:11,305 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:11,354 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:11,361 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:11,418 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:13,126 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:14,203 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:15,914 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:50:16,373 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:17,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:50:17,345 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:21,057 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:50:21,523 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:21,682 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:50:22,591 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:24,230 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:50:24,542 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:24,773 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:28,235 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:50:28,362 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:28,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:28,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:28,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:28,369 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:28,369 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:50:28,831 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:28,955 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:28,998 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:29,196 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:29,854 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:34,005 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:35,902 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:35,903 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:35,903 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:35,904 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:35,904 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:50:36,302 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:36,455 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:36,475 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:37,417 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:37,536 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:39,487 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:42,071 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:50:42,510 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:42,933 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:42,943 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:42,944 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:42,945 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:42,945 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:42,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:50:43,313 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:44,167 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:44,630 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:44,971 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:50:45,368 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:50:45,408 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:45,474 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:45,892 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:50:46,722 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:48,589 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:50:51,441 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:50:51,853 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:54,354 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:50:54,759 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:50:57,344 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:50:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:50:57,451 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:50:57,995 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:00,666 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:51:01,280 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:02,891 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:51:04,838 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:04,847 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:06,809 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:51:07,327 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:08,481 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:51:08,708 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:11,012 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:11,478 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:11,478 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:51:13,991 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:14,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:51:14,405 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:15,791 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:19,803 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:51:21,957 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:21,962 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:51:22,922 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:25,062 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:51:25,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:51:25,859 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:51:27,900 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:34,909 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:51:35,308 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:36,502 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:51:36,630 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:36,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:51:37,321 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:39,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:39,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:51:40,092 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:40,192 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:41,890 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:42,653 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:51:45,318 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:47,463 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:48,631 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:48,632 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 10:51:49,057 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:49,156 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:53,482 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:53,483 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:51:54,068 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:54,106 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:55,726 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:51:56,250 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:57,244 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:51:57,275 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:57,280 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:57,306 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:51:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:51:57,307 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:51:57,843 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:58,104 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:58,638 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:51:58,756 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:02,678 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:02,727 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:52:02,992 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:52:04,266 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:06,609 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:06,670 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:52:07,528 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:08,475 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:11,133 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:52:12,743 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:13,846 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:52:13,911 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:52:14,292 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:52:15,186 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:19,545 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:21,048 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:52:21,166 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:22,064 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:24,886 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:52:37,378 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:37,397 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:37,398 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:37,398 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:37,399 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:37,399 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:52:37,776 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:38,107 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:38,203 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:39,427 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:39,610 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:39,675 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:52:39,676 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:52:39,676 - core.experiment_manager - INFO - Phase 2 completed with consensus on Maximizing the floor\n",
      "2025-08-10 10:52:39,679 - core.experiment_manager - INFO - Experiment eedc776f-49e5-4258-9ebb-8b712c241786 completed successfully in 980.28 seconds\n",
      "2025-08-10 10:52:39,686 - core.experiment_manager - INFO - Results saved to: hypothesis_2_&_4/logs/config_09_results_20250810_103619.json\n",
      "2025-08-10 10:52:39,688 - utils.experiment_runner - INFO - Completed experiment: config_09 -> hypothesis_2_&_4/logs/config_09_results_20250810_103619.json\n",
      "2025-08-10 10:52:40,693 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:52:43,268 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:43,285 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:43,286 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:43,286 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:43,287 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:43,287 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:52:43,668 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:44,798 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:44,944 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:45,415 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:45,802 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:46,658 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:49,600 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:49,600 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:49,601 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:49,601 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:49,601 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:49,730 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:52:50,025 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:50,054 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:52:50,536 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:51,047 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:51,108 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:52,070 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:52,228 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:52,319 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:52,689 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:52:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:52:55,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:52:58,029 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:52:58,525 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:53:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:53:09,739 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:53:10,373 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:53:14,909 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:53:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:53:14,918 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:53:15,227 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:53:15,462 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:53:17,322 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:53:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:53:33,044 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:53:34,121 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:53:36,179 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:53:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:53:41,896 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:53:42,310 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:53:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:53:43,784 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:53:45,341 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:53:47,642 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:53:48,228 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:53:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:53:48,239 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:53:48,978 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:53:57,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:53:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:53:57,900 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:53:58,529 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:01,555 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:02,125 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:03,649 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:03,653 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:54:05,391 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:09,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:54:11,239 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:11,247 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:11,748 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:14,820 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:16,437 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:17,168 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:18,963 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:19,178 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:20,124 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:20,799 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:54:21,089 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:27,674 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:27,677 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:28,216 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:28,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:54:29,455 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:54:29,456 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:29,461 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:29,461 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:29,461 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:29,461 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:29,462 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:54:29,966 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:30,172 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:30,232 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:30,570 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:30,844 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:31,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:31,539 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:34,053 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:34,710 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:35,497 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:35,806 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:35,807 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:54:38,260 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:38,395 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:39,452 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:39,459 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:40,345 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:41,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:54:43,793 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:44,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:54:44,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:45,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:45,991 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:47,170 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:47,392 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:54:47,796 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:48,699 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 10:54:49,221 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:49,720 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:50,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:50,125 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:54:50,626 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:51,570 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:54:51,596 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:51,980 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:54:56,683 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:56,691 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:56,692 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:56,692 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:56,693 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:54:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:54:56,693 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:54:57,000 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:57,016 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:58,072 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:54:58,202 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:00,375 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:55:00,741 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:55:01,130 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:55:01,252 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:01,679 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:03,233 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:04,887 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:04,888 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:55:04,939 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:06,217 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:06,329 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:07,845 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:08,176 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:08,994 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:55:09,756 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:10,699 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:16,331 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:55:16,673 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:20,048 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 10:55:21,619 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:22,708 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:55:28,438 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:31,308 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:55:33,863 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:33,913 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:55:38,302 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:55:38,303 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 10:55:38,303 - core.experiment_manager - INFO - Phase 2 completed with consensus on Maximizing the floor\n",
      "2025-08-10 10:55:38,306 - core.experiment_manager - INFO - Experiment 0cb973e7-4c30-43e4-8b28-85b05f67c246 completed successfully in 1159.60 seconds\n",
      "2025-08-10 10:55:38,329 - core.experiment_manager - INFO - Results saved to: hypothesis_2_&_4/logs/config_01_results_20250810_103618.json\n",
      "2025-08-10 10:55:38,333 - utils.experiment_runner - INFO - Completed experiment: config_01 -> hypothesis_2_&_4/logs/config_01_results_20250810_103618.json\n",
      "2025-08-10 10:55:39,339 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:40,245 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:55:44,652 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:46,925 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:55:50,754 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:50,800 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:50,801 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:50,802 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:50,802 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:50,803 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:50,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:55:51,381 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:51,872 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:52,046 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:52,075 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:52,204 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:52,880 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:55:53,007 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:55:58,292 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:55:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:55:58,760 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:55:59,260 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:03,969 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:07,362 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:56:07,860 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:10,535 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:10,681 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:56:11,192 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:14,618 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "\u001b[92m10:56:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:14,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:14,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 10:56:14,644 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:14,646 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:14,647 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:56:15,336 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:15,340 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:15,757 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:15,794 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:16,157 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:16,779 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:16,970 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:17,214 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:22,833 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:23,758 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:56:24,451 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:28,340 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:29,436 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:56:31,063 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:34,591 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 10:56:34,644 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:35,099 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:36,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:38,176 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:38,182 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:38,183 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:38,183 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:38,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:38,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:56:38,520 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:38,626 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:39,378 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:40,659 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:41,058 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:43,804 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:44,114 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:44,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:44,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:44,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:44,125 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:44,126 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 10:56:44,604 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:44,717 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:44,751 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:45,227 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:56:46,262 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:46,459 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:51,281 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:56:52,340 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:52,713 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-10 10:56:54,763 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:56:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:56:58,986 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:57:00,507 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:57:03,273 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:57:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:57:20,062 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:57:22,633 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:57:24,144 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:57:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:57:32,668 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:57:33,567 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:57:34,736 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:57:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:57:46,593 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 10:57:49,510 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:57:50,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:57:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:57:54,047 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 10:57:55,308 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 10:57:55,967 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the experiments\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[43mrun_experiments_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_PARALLEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParallel execution completed at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/utils/experiment_runner.py:337\u001b[39m, in \u001b[36mrun_experiments_parallel\u001b[39m\u001b[34m(config_files, max_parallel, output_dir, verbose)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiments_parallel\u001b[39m(\n\u001b[32m    320\u001b[39m     config_files: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    321\u001b[39m     max_parallel: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    322\u001b[39m     output_dir: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mhypothesis_2_&_4/logs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    323\u001b[39m     verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    324\u001b[39m ) -> List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    325\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    Synchronous wrapper for running experiments in parallel.\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    335\u001b[39m \u001b[33;03m    - List of experiment results dictionaries\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_experiments_parallel_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28mself\u001b[39m._selector.control(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 10:58:06,975 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    }
   ],
   "source": [
    "# Set up execution parameters\n",
    "MAX_PARALLEL = 5  # Number of experiments to run concurrently\n",
    "OUTPUT_DIR = \"hypothesis_2_&_4/logs\"  # Directory to save results\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Starting parallel execution of {len(config_files)} experiments\")\n",
    "print(f\"Maximum parallel experiments: {MAX_PARALLEL}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run the experiments\n",
    "try:\n",
    "    results = run_experiments_parallel(\n",
    "        config_files=config_files,\n",
    "        max_parallel=MAX_PARALLEL,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Parallel execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nParallel execution failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4ds2chj49b",
   "metadata": {},
   "source": [
    "## Quick Results Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
