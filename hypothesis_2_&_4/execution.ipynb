{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d36bcd1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6623a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import random\n",
    "import yaml\n",
    "# Change to project root directory\n",
    "project_root = '/Users/lucasmuller/Desktop/Githubg/Rawls_v3'\n",
    "os.chdir(project_root)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils.experiment_runner import generate_random_config, generate_and_save_configs, run_experiment, run_experiments_parallel\n",
    "\n",
    "from typing import Union, List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d1ece",
   "metadata": {},
   "source": [
    "# Parameter Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [\n",
    "    \"x-ai/grok-4\",\n",
    "    \"google/gemini-2.5-flash\",\n",
    "    \"google/gemini-2.5-pro\",\n",
    "    \"meta-llama/llama-4-maverick\",\n",
    "    \"anthropic/claude-sonnet-4\",\n",
    "    \"openai/gpt-4.1-mini\",\n",
    "    \"openai/gpt-4.1\",\n",
    "    \"deepseek/deepseek-chat-v3-0324\",\n",
    "    \"qwen/qwen3-coder\",\n",
    "    \"moonshotai/kimi-k2\"\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"gpt-4.1-nano\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06703f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_config(\n",
    "      num_agents: int = 3,\n",
    "      num_rounds: int = 10,\n",
    "      personality: str = \"You are an american college student.\",\n",
    "      models: Union[str, List[str]] = models,\n",
    "      temperature: Union[float, tuple] = (0.0, 1.0),\n",
    "      memory_limit: int = 50000,\n",
    "      reasoning_enabled: bool = True,\n",
    "      utility_model: str = \"openai/gpt-4.1-nano\",\n",
    "      language: str = \"English\",\n",
    "      distribution_range_phase1: List[float] = [0.5, 2.0],\n",
    "      distribution_range_phase2: List[float] = [0.5, 2.0]\n",
    "  ) -> Dict[str, Any]:\n",
    "      \"\"\"\n",
    "      Generate a random configuration for the Frohlich Experiment.\n",
    "      \n",
    "      Parameters:\n",
    "      - num_agents: Number of participant agents\n",
    "      - num_rounds: Number of Phase 2 rounds\n",
    "      - personality: Single personality description used for all agents\n",
    "      - models: Single model string or list to randomly select from\n",
    "      - temperature: Fixed temperature (float) or range (tuple) for random selection\n",
    "      - memory_limit: Memory character limit for agents\n",
    "      - reasoning_enabled: Whether agents use reasoning\n",
    "      - utility_model: Model for utility agent\n",
    "      - language: Language for prompts (\"English\", \"Spanish\", \"Mandarin\")\n",
    "      - distribution_range_phase1/2: Income distribution multiplier ranges\n",
    "      \n",
    "      Returns:\n",
    "      - Dictionary representing the experiment configuration\n",
    "      \"\"\"\n",
    "\n",
    "      # Generate agents\n",
    "      agents = []\n",
    "      for i in range(num_agents):\n",
    "          # Generate agent name as Agent_1, Agent_2, etc.\n",
    "          agent_name = f\"Agent_{i + 1}\"\n",
    "\n",
    "          # Select model\n",
    "          if isinstance(models, list):\n",
    "              selected_model = random.choice(models)\n",
    "          else:\n",
    "              selected_model = models\n",
    "\n",
    "          # Select temperature\n",
    "          if isinstance(temperature, tuple) and len(temperature) == 2:\n",
    "              selected_temp = round(random.uniform(temperature[0], temperature[1]), 2)\n",
    "          else:\n",
    "              selected_temp = float(temperature)\n",
    "\n",
    "          agent_config = {\n",
    "              \"name\": agent_name,\n",
    "              \"personality\": personality,\n",
    "              \"model\": selected_model,\n",
    "              \"temperature\": selected_temp,\n",
    "              \"memory_character_limit\": memory_limit,\n",
    "              \"reasoning_enabled\": reasoning_enabled\n",
    "          }\n",
    "          agents.append(agent_config)\n",
    "\n",
    "      # Build complete configuration\n",
    "      config = {\n",
    "          \"language\": language,\n",
    "          \"agents\": agents,\n",
    "          \"utility_agent_model\": utility_model,\n",
    "          \"phase2_rounds\": num_rounds,\n",
    "          \"distribution_range_phase1\": distribution_range_phase1,\n",
    "          \"distribution_range_phase2\": distribution_range_phase2\n",
    "      }\n",
    "\n",
    "      return config\n",
    "\n",
    "def generate_and_save_configs(\n",
    "      num_configs: int = 10,\n",
    "      save_path: str = \"hypothesis_2_&_4/configs/condition_1\",\n",
    "      **config_params\n",
    "  ):\n",
    "      \"\"\"\n",
    "      Generate multiple random configurations and save them as YAML files.\n",
    "      \n",
    "      Parameters:\n",
    "      - num_configs: Number of configurations to generate\n",
    "      - save_path: Path to save the configuration files\n",
    "      - **config_params: Parameters to pass to generate_random_config()\n",
    "      \n",
    "      Returns:\n",
    "      - List of generated configurations\n",
    "      \"\"\"\n",
    "\n",
    "      # Create directory if it doesn't exist\n",
    "      os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "      configs = []\n",
    "      for i in range(num_configs):\n",
    "          # Generate random config\n",
    "          config = generate_random_config(**config_params)\n",
    "          configs.append(config)\n",
    "\n",
    "          # Save to YAML file\n",
    "          filename = f\"config_{i+1:02d}.yaml\"\n",
    "          filepath = os.path.join(save_path, filename)\n",
    "\n",
    "          with open(filepath, 'w') as f:\n",
    "              yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "          print(f\"Saved {filename}\")\n",
    "\n",
    "      print(f\"Generated and saved {num_configs} configurations in {save_path}\")\n",
    "      return configs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe746ee",
   "metadata": {},
   "source": [
    "# 1. Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c5e4f",
   "metadata": {},
   "source": [
    "## Generating Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7773f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved config_01.yaml\n",
      "Saved config_02.yaml\n",
      "Saved config_03.yaml\n",
      "Saved config_04.yaml\n",
      "Generated and saved 4 configurations in hypothesis_2_&_4/configs/condition_1\n"
     ]
    }
   ],
   "source": [
    "# Generate configs 1. \n",
    "# Commenteed out to avoid running if accidentally clicked run all \n",
    "configs = generate_and_save_configs(\n",
    "    num_configs=4,\n",
    "    num_agents=5,\n",
    "    personality=\"You are an american college student.\",\n",
    "    models=models,\n",
    "    temperature=(0),\n",
    "    num_rounds=20,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ba09a",
   "metadata": {},
   "source": [
    "## Running Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ef0d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 configuration files:\n",
      "   1. config_01.yaml\n",
      "   2. config_02.yaml\n",
      "   3. config_03.yaml\n",
      "   4. config_04.yaml\n",
      "\n",
      "Total configurations to run: 4\n"
     ]
    }
   ],
   "source": [
    "# Get all config files from condition_1\n",
    "config_dir = \"hypothesis_2_&_4/configs/condition_1\"\n",
    "config_files = sorted(glob.glob(f\"{config_dir}/config_*.yaml\"))\n",
    "\n",
    "print(f\"Found {len(config_files)} configuration files:\")\n",
    "for i, config_file in enumerate(config_files, 1):\n",
    "    config_name = Path(config_file).name\n",
    "    print(f\"  {i:2d}. {config_name}\")\n",
    "\n",
    "# Display the total\n",
    "print(f\"\\nTotal configurations to run: {len(config_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c1784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel execution of 4 experiments\n",
      "Maximum parallel experiments: 2\n",
      "Output directory: hypothesis_2_&_4/logs\n",
      "Start time: 2025-08-11 00:09:27\n",
      "\n",
      "================================================================================\n",
      "\n",
      "2025-08-11 00:09:27,940 - utils.experiment_runner - INFO - Loaded 4 configurations\n",
      "2025-08-11 00:09:27,940 - utils.experiment_runner - INFO - Running with max 2 parallel experiments\n",
      "2025-08-11 00:09:27,941 - utils.experiment_runner - INFO - Starting parallel experiment execution...\n",
      "2025-08-11 00:09:27,941 - utils.experiment_runner - INFO - Starting experiment: config_01\n",
      "2025-08-11 00:09:27,947 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-11 00:09:27,949 - core.experiment_manager - INFO - Created participant: Agent_1 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-11 00:09:27,949 - core.experiment_manager - INFO - Created participant: Agent_2 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-11 00:09:27,950 - core.experiment_manager - INFO - Created participant: Agent_3 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-11 00:09:27,950 - core.experiment_manager - INFO - Created participant: Agent_4 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-11 00:09:27,950 - core.experiment_manager - INFO - Created participant: Agent_5 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-11 00:09:27,952 - utils.language_manager - INFO - Loaded translations for English\n",
      "2025-08-11 00:09:27,957 - core.experiment_manager - INFO - Starting Phase 1 for experiment 104db11f-9141-4750-920b-bce59cd3c8ae\n",
      "2025-08-11 00:09:27,958 - utils.experiment_runner - INFO - Starting experiment: config_02\n",
      "2025-08-11 00:09:27,962 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-11 00:09:27,962 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-11 00:09:27,963 - core.experiment_manager - INFO - Created participant: Agent_2 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-11 00:09:27,963 - core.experiment_manager - INFO - Created participant: Agent_3 (x-ai/grok-4, temp=0.0)\n",
      "2025-08-11 00:09:27,963 - core.experiment_manager - INFO - Created participant: Agent_4 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-11 00:09:27,964 - core.experiment_manager - INFO - Created participant: Agent_5 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-11 00:09:27,964 - core.experiment_manager - INFO - Starting Phase 1 for experiment b82ab673-4cab-45fa-bfcf-dc519c04e611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,975 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,977 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,978 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,978 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,979 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,980 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,982 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,984 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,984 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:27,986 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:09:28,818 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:28,821 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:28,823 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:28,939 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:28,941 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:29,068 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:29,850 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:30,233 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:30,790 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:30,895 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:09:30,981 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:31,325 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:32,977 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:33,218 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:09:33,505 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:33,622 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:09:33,709 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:33,888 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:34,015 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:34,499 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:09:34,747 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:35,031 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:36,436 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:09:37,076 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:37,627 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:37,697 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:09:38,215 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:38,375 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:39,159 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:39,224 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:09:39,706 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:39,789 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:09:39,962 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:40,425 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:41,005 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:41,614 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:09:41,985 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:42,192 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:43,742 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:43,799 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:44,051 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:09:44,324 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:44,335 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:45,087 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:45,548 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:09:46,028 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:46,732 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:09:47,238 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:48,983 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:48,996 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:09:49,446 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:49,530 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:09:49,943 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:50,564 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:50,619 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:09:50,881 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:51,047 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:51,188 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:53,140 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:53,335 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:53,567 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:09:53,876 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:54,422 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:09:54,754 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:54,832 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:55,318 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:09:55,899 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:55,944 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:09:55,952 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:56,033 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:56,385 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:09:56,433 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:56,537 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:56,889 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:56,946 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:09:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:09:57,286 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:09:58,379 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:09:59,056 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:00,069 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:10:00,679 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:01,139 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:10:01,749 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:02,478 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:10:02,826 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:04,752 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:10:04,968 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:05,136 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:10:05,293 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:05,402 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:05,494 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:05,679 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:05,692 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:05,756 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:05,933 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:10:06,060 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:06,147 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:06,177 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:07,248 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:07,804 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:10:09,469 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:09,545 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:10,689 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:11,116 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:11,270 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:11,917 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:12,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:12,468 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:13,384 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:13,622 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:14,003 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:14,195 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:14,605 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:14,671 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:14,754 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:15,169 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:15,265 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:10:15,281 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:16,507 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:16,740 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:10:17,221 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:17,284 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:10:17,738 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:10:17,787 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:17,999 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:19,481 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:19,786 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:20,071 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:20,277 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:20,333 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:20,901 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:21,736 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:23,200 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:23,966 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:24,044 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:10:24,363 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:24,432 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:24,623 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:24,638 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:10:24,782 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:24,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:24,905 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:25,771 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:26,066 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:10:26,186 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:26,287 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:26,435 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:28,280 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:28,421 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:10:28,974 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:29,150 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:10:29,202 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:29,243 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:29,292 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:29,463 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:10:29,759 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:29,936 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:30,000 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:30,673 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:30,908 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:31,058 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:10:31,592 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:10:31,827 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:32,256 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:32,625 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:10:32,738 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:33,150 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:36,103 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:36,763 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:37,309 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:39,282 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:10:39,820 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:39,919 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:40,234 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:40,258 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:40,438 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-11 00:10:40,668 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:40,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:41,318 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:42,759 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:43,357 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:43,997 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:44,481 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:44,625 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:44,959 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:10:44,979 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:45,179 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:45,561 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:46,228 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:46,487 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:46,785 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:48,382 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:10:49,271 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:49,520 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:10:50,042 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:50,105 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:50,208 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:50,400 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:51,179 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:51,624 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:54,141 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:10:55,054 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:55,600 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:55,788 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:10:56,034 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:10:56,179 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:58,107 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:10:58,600 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:58,696 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:10:58,778 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:10:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:10:59,838 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:00,203 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:00,771 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:00,881 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:11:00,895 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:01,086 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:11:01,388 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:11:01,389 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:03,213 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:03,720 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:11:04,489 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:07,128 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:07,186 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:07,633 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:11:07,994 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:09,048 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:09,517 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:10,919 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:11:11,463 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:11,894 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:12,256 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:12,792 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:12,994 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:11:13,541 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:17,532 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:17,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:18,047 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:11:18,049 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:18,121 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:18,601 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:19,026 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:19,681 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:21,829 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:22,389 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:22,708 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:22,733 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:11:23,506 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:24,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:11:24,734 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:24,789 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:11:25,377 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:28,336 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:29,976 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:30,187 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:30,699 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:30,862 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:30,996 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:11:33,810 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:36,556 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:37,873 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:38,108 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:38,201 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:38,308 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:38,728 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:40,469 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:41,176 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:42,313 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:42,546 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:11:43,082 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:11:48,515 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:48,959 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:49,598 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:49,914 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:50,055 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:50,504 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:50,606 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:51,512 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:11:52,685 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:11:54,479 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:11:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:11:59,042 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:11:59,784 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:12:00,269 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:00,570 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:12:01,255 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:12:05,964 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:06,617 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:08,527 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:12:08,645 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:12:08,824 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:12:11,639 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:14,944 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:12:17,389 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:12:17,446 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:18,003 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:12:18,265 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:18,789 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:12:19,711 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:21,107 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:12:22,340 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:12:23,628 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:24,525 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:12:25,386 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:28,507 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:29,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:12:29,450 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:12:29,749 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:12:29,962 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:12:30,813 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:12:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:12:40,478 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:12:42,895 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:12:46,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:03,188 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:13:03,652 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:06,352 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:13:06,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:13:07,023 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:07,037 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:13:10,082 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:13:12,402 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:17,198 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:18,181 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:13:18,246 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:13:18,868 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:13:19,240 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:13:19,370 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:31,732 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:13:32,417 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:13:34,763 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:38,075 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:13:38,354 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:13:40,463 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:40,967 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:13:43,228 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:45,042 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:13:46,197 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:13:49,158 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:13:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:13:50,539 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:13:51,211 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:13:51,987 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:14:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:14:00,205 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:14:01,202 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:14:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:14:02,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:14:02,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:14:03,126 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:14:03,659 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:14:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:14:06,241 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:14:08,577 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:14:08,675 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:14:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:14:26,084 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:14:28,828 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:14:29,620 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:14:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:14:39,196 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:14:39,920 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:14:40,174 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:14:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:14:49,353 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:14:50,323 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:14:50,948 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:14:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:14:54,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:14:56,721 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:14:56,993 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:15:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:15:16,090 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:15:17,992 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:15:19,401 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:15:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:15:25,180 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-11 00:15:25,580 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:15:28,777 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:15:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:15:35,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:15:36,287 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:15:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:15:38,843 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-11 00:15:39,375 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:15:41,330 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:00,464 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:16:01,689 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-11 00:16:01,689 - core.experiment_manager - INFO - Agent_1: $7.69 earned\n",
      "2025-08-11 00:16:01,690 - core.experiment_manager - INFO - Agent_2: $10.50 earned\n",
      "2025-08-11 00:16:01,690 - core.experiment_manager - INFO - Agent_3: $11.41 earned\n",
      "2025-08-11 00:16:01,690 - core.experiment_manager - INFO - Agent_4: $12.54 earned\n",
      "2025-08-11 00:16:01,691 - core.experiment_manager - INFO - Agent_5: $12.23 earned\n",
      "2025-08-11 00:16:01,691 - core.experiment_manager - INFO - Starting Phase 2 for experiment 104db11f-9141-4750-920b-bce59cd3c8ae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:01,694 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:16:02,364 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:05,912 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:06,264 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:16:06,727 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:08,779 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-11 00:16:09,402 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:11,618 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:16:17,036 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:16:19,241 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-11 00:16:19,244 - openai._base_client - INFO - Retrying request to /responses in 0.422133 seconds\n",
      "2025-08-11 00:16:24,688 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-11 00:16:24,690 - openai._base_client - INFO - Retrying request to /responses in 0.903711 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:26,948 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:16:27,739 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:28,250 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:16:30,095 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-11 00:16:30,100 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_cc09a8e32294a3ea91e2a6a1c1becae4)\n",
      "2025-08-11 00:16:30,102 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_01.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2025-08-11 00:16:30,103 - utils.experiment_runner - INFO - Starting experiment: config_03\n",
      "2025-08-11 00:16:30,115 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-11 00:16:30,116 - core.experiment_manager - INFO - Created participant: Agent_1 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-11 00:16:30,116 - core.experiment_manager - INFO - Created participant: Agent_2 (meta-llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-11 00:16:30,116 - core.experiment_manager - INFO - Created participant: Agent_3 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-11 00:16:30,117 - core.experiment_manager - INFO - Created participant: Agent_4 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-11 00:16:30,117 - core.experiment_manager - INFO - Created participant: Agent_5 (x-ai/grok-4, temp=0.0)\n",
      "2025-08-11 00:16:30,118 - core.experiment_manager - INFO - Starting Phase 1 for experiment 815bf5cc-9de6-4ad0-9d63-10713c46b425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:30,122 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:30,122 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:30,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:30,125 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:30,125 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:16:30,434 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:30,438 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:30,770 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:31,005 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:31,334 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:16:31,703 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:31,704 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:33,843 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:16:33,928 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:16:34,470 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:34,985 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:16:35,257 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:35,485 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:37,493 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:16:38,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:38,741 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:38,837 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:16:39,615 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:40,177 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:40,499 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:16:40,940 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:43,273 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:43,807 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:16:44,341 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:44,590 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:16:45,130 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:45,665 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:16:46,075 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:16:46,390 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:16:46,481 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:46,893 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:16:47,408 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:48,667 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:16:48,929 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:50,280 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:16:51,091 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:51,236 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:16:51,771 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:52,178 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:52,878 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:16:53,395 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:53,543 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:16:53,948 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:55,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:16:55,923 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:16:57,901 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:58,100 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:16:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:16:59,596 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:00,008 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:00,762 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:01,145 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:01,173 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:17:01,332 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:01,510 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:01,860 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:02,088 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:03,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:05,259 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:05,780 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:08,311 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:17:09,184 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:17:09,624 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:10,434 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:17:10,957 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:12,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:12,421 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:13,158 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:13,615 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:17:13,700 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:14,124 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:15,027 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:16,083 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:16,123 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:16,482 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:16,762 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:18,810 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:17:19,493 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:20,806 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:22,066 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:22,892 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:24,252 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:24,388 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:17:24,802 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:24,884 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:26,589 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:26,763 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:27,133 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:17:27,919 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:28,288 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:31,248 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:31,467 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:31,768 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:32,489 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:32,515 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:32,825 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:33,105 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:34,029 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:17:36,739 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:38,021 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:38,196 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:17:38,482 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:38,707 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:39,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:39,852 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:17:40,472 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:41,384 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:41,514 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-11 00:17:41,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:43,259 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:17:44,159 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:44,660 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:45,533 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:46,543 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:17:47,242 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:47,440 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:50,389 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:58,061 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:17:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:17:58,113 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:17:58,511 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:17:58,826 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:01,173 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:03,193 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:18:03,718 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:04,694 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:18:05,281 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:05,431 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:18:05,990 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:06,957 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:08,823 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:18:09,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:09,826 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:18:10,405 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:12,643 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:19,539 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:18:20,108 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:23,933 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:24,511 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:25,824 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:26,380 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:18:26,709 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:27,139 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:27,142 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:29,580 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:30,210 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:18:30,842 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:34,045 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:18:34,614 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:35,080 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:18:40,807 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:18:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:18:48,403 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-11 00:18:49,168 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:18:51,588 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:19:02,425 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:19:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:19:07,376 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:19:08,000 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:19:08,237 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:19:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:19:13,467 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:19:14,756 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:19:18,850 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:19:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:19:23,983 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:19:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:19:24,661 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:19:24,907 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:19:25,167 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:19:29,703 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:19:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:19:35,369 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:19:38,071 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:19:40,469 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:19:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:19:59,781 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:20:01,392 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:20:04,933 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:20:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:20:27,308 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:20:27,873 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:20:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:20:29,253 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:20:29,868 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:20:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:20:32,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-11 00:20:32,599 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:20:33,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:20:33,458 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:20:54,185 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n",
      "2025-08-11 00:21:12,913 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-11 00:21:12,916 - core.experiment_manager - INFO - Agent_1: $13.29 earned\n",
      "2025-08-11 00:21:12,916 - core.experiment_manager - INFO - Agent_2: $10.96 earned\n",
      "2025-08-11 00:21:12,917 - core.experiment_manager - INFO - Agent_3: $7.49 earned\n",
      "2025-08-11 00:21:12,917 - core.experiment_manager - INFO - Agent_4: $8.65 earned\n",
      "2025-08-11 00:21:12,918 - core.experiment_manager - INFO - Agent_5: $15.63 earned\n",
      "2025-08-11 00:21:12,918 - core.experiment_manager - INFO - Starting Phase 2 for experiment b82ab673-4cab-45fa-bfcf-dc519c04e611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:21:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:21:12,929 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:21:14,314 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:21:15,017 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m00:21:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:21:16,913 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-11 00:21:17,476 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-11 00:21:20,912 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the experiments\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[43mrun_experiments_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_PARALLEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParallel execution completed at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/utils/experiment_runner.py:338\u001b[39m, in \u001b[36mrun_experiments_parallel\u001b[39m\u001b[34m(config_files, max_parallel, output_dir, verbose)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiments_parallel\u001b[39m(\n\u001b[32m    321\u001b[39m     config_files: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    322\u001b[39m     max_parallel: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    323\u001b[39m     output_dir: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mhypothesis_2_&_4/logs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    324\u001b[39m     verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    325\u001b[39m ) -> List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    Synchronous wrapper for running experiments in parallel.\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m \u001b[33;03m    - List of experiment results dictionaries\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_experiments_parallel_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28mself\u001b[39m._selector.control(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 00:21:42,172 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest \"HTTP/1.1 204 No Content\"\n"
     ]
    }
   ],
   "source": [
    "# Set up execution parameters\n",
    "MAX_PARALLEL = 2 # Number of experiments to run concurrently\n",
    "OUTPUT_DIR = \"hypothesis_2_&_4/logs\"  # Directory to save results\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Starting parallel execution of {len(config_files)} experiments\")\n",
    "print(f\"Maximum parallel experiments: {MAX_PARALLEL}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run the experiments\n",
    "try:\n",
    "    results = run_experiments_parallel(\n",
    "        config_files=config_files,\n",
    "        max_parallel=MAX_PARALLEL,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Parallel execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nParallel execution failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115231be",
   "metadata": {},
   "source": [
    "# Example: Run Config_01 from Condition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5mhum5el7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "- Language: English\n",
      "- Number of agents: 5\n",
      "- Phase 2 rounds: 20\n",
      "- Utility agent model: gpt-4.1-mini\n",
      "\n",
      "Agent details:\n",
      "  - Agent_1: google/gemini-2.5-pro (temp=0.0)\n",
      "  - Agent_2: google/gemini-2.5-pro (temp=0.0)\n",
      "  - Agent_3: openai/gpt-4.1-mini (temp=0.0)\n",
      "  - Agent_4: anthropic/claude-sonnet-4 (temp=0.0)\n",
      "  - Agent_5: anthropic/claude-sonnet-4 (temp=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Load config_01.yaml\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "config_path = 'hypothesis_2_&_4/configs/condition_1/config_01.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(f\"- Language: {config_dict['language']}\")\n",
    "print(f\"- Number of agents: {len(config_dict['agents'])}\")\n",
    "print(f\"- Phase 2 rounds: {config_dict['phase2_rounds']}\")\n",
    "print(f\"- Utility agent model: {config_dict['utility_agent_model']}\")\n",
    "print(\"\\nAgent details:\")\n",
    "for agent in config_dict['agents']:\n",
    "    print(f\"  - {agent['name']}: {agent['model']} (temp={agent['temperature']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sqn0t3lc8rs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with config_01...\n",
      "Results will be saved to: hypothesis_2_&_4/logs/config_01_results_20250810_103542.json\n",
      "\n",
      "============================================================\n",
      "\n",
      "Experiment failed with error: name 'config_dict' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/wf/_h59fnv53s7476fhw5sn5smh0000gn/T/ipykernel_65546/2338803112.py\", line 12, in <module>\n",
      "    config_dict=config_dict,\n",
      "                ^^^^^^^^^^^\n",
      "NameError: name 'config_dict' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Create output path with timestamp in logs folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = f\"hypothesis_2_&_4/logs/config_01_results_{timestamp}.json\"\n",
    "\n",
    "print(f\"Running experiment with config_01...\")\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Run the experiment\n",
    "try:\n",
    "    results = run_experiment(\n",
    "        config_dict=config_dict,\n",
    "        output_path=output_path,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nExperiment failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41757a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 configuration files:\n",
      "   1. config_01.yaml\n",
      "   2. config_02.yaml\n",
      "   3. config_03.yaml\n",
      "   4. config_04.yaml\n",
      "   5. config_05.yaml\n",
      "   6. config_06.yaml\n",
      "   7. config_07.yaml\n",
      "   8. config_08.yaml\n",
      "   9. config_09.yaml\n",
      "  10. config_10.yaml\n",
      "\n",
      "Total configurations to run: 10\n"
     ]
    }
   ],
   "source": [
    "# Get all config files from condition_1\n",
    "config_dir = \"hypothesis_2_&_4/configs/condition_1\"\n",
    "config_files = sorted(glob.glob(f\"{config_dir}/config_*.yaml\"))\n",
    "\n",
    "print(f\"Found {len(config_files)} configuration files:\")\n",
    "for i, config_file in enumerate(config_files, 1):\n",
    "    config_name = Path(config_file).name\n",
    "    print(f\"  {i:2d}. {config_name}\")\n",
    "\n",
    "# Display the total\n",
    "print(f\"\\nTotal configurations to run: {len(config_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel execution of 10 experiments\n",
      "Maximum parallel experiments: 5\n",
      "Output directory: hypothesis_2_&_4/logs\n",
      "Start time: 2025-08-10 23:21:42\n",
      "\n",
      "================================================================================\n",
      "\n",
      "2025-08-10 23:21:42,235 - utils.experiment_runner - INFO - Loaded 10 configurations\n",
      "2025-08-10 23:21:42,236 - utils.experiment_runner - INFO - Running with max 5 parallel experiments\n",
      "2025-08-10 23:21:42,236 - utils.experiment_runner - INFO - Starting parallel experiment execution...\n",
      "2025-08-10 23:21:42,238 - utils.experiment_runner - INFO - Starting experiment: config_01\n",
      "2025-08-10 23:21:42,250 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,257 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:42,258 - core.experiment_manager - INFO - Created participant: Agent_2 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:42,258 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,258 - core.experiment_manager - INFO - Created participant: Agent_4 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,258 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,263 - core.experiment_manager - INFO - Starting Phase 1 for experiment 637e3be6-986a-452f-916b-5853f5539f4e\n",
      "2025-08-10 23:21:42,264 - utils.experiment_runner - INFO - Starting experiment: config_02\n",
      "2025-08-10 23:21:42,268 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,268 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,269 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 23:21:42,269 - core.experiment_manager - INFO - Created participant: Agent_3 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,269 - core.experiment_manager - INFO - Created participant: Agent_4 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,270 - core.experiment_manager - INFO - Created participant: Agent_5 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,270 - core.experiment_manager - INFO - Starting Phase 1 for experiment 90755503-f1ab-44c8-86e6-89d0480ae5b7\n",
      "2025-08-10 23:21:42,272 - utils.experiment_runner - INFO - Starting experiment: config_03\n",
      "2025-08-10 23:21:42,275 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,276 - core.experiment_manager - INFO - Created participant: Agent_1 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-10 23:21:42,276 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,276 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,277 - core.experiment_manager - INFO - Created participant: Agent_4 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,277 - core.experiment_manager - INFO - Created participant: Agent_5 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 23:21:42,278 - core.experiment_manager - INFO - Starting Phase 1 for experiment 85dfa257-dff6-41ac-9170-0b4d17beec88\n",
      "2025-08-10 23:21:42,278 - utils.experiment_runner - INFO - Starting experiment: config_04\n",
      "2025-08-10 23:21:42,282 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,282 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 23:21:42,282 - core.experiment_manager - INFO - Created participant: Agent_2 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,282 - core.experiment_manager - INFO - Created participant: Agent_3 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,282 - core.experiment_manager - INFO - Created participant: Agent_4 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:42,283 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,283 - core.experiment_manager - INFO - Starting Phase 1 for experiment 2b47e7d1-1bb7-4fd3-abb2-a919c557a913\n",
      "2025-08-10 23:21:42,283 - utils.experiment_runner - INFO - Starting experiment: config_05\n",
      "2025-08-10 23:21:42,289 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,289 - core.experiment_manager - INFO - Created participant: Agent_1 (qwen/qwen3-coder, temp=0.0)\n",
      "2025-08-10 23:21:42,289 - core.experiment_manager - INFO - Created participant: Agent_2 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,290 - core.experiment_manager - INFO - Created participant: Agent_3 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,290 - core.experiment_manager - INFO - Created participant: Agent_4 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,291 - core.experiment_manager - INFO - Created participant: Agent_5 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,291 - core.experiment_manager - INFO - Starting Phase 1 for experiment ddaf22ba-7382-48bb-8d8e-7faded780afe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,339 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,340 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,344 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,344 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,344 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:21:42,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,345 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,346 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,350 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,351 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,360 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,361 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,364 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,366 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,367 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,372 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,374 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-10 23:21:42,376 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n",
      "2025-08-10 23:21:42,709 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,734 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,756 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_04.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:42,757 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,765 - utils.experiment_runner - INFO - Starting experiment: config_06\n",
      "2025-08-10 23:21:42,769 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,769 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,773 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 23:21:42,777 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,777 - core.experiment_manager - INFO - Created participant: Agent_4 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,778 - core.experiment_manager - INFO - Created participant: Agent_5 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,780 - core.experiment_manager - INFO - Starting Phase 1 for experiment 11de5f06-7c1c-4d3f-9173-30228b75476a\n",
      "2025-08-10 23:21:42,797 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_05.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:42,806 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_02.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:42,811 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,827 - utils.experiment_runner - INFO - Starting experiment: config_07\n",
      "2025-08-10 23:21:42,831 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,831 - core.experiment_manager - INFO - Created participant: Agent_1 (x-ai/grok-4, temp=0.0)\n",
      "2025-08-10 23:21:42,832 - core.experiment_manager - INFO - Created participant: Agent_2 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,832 - core.experiment_manager - INFO - Created participant: Agent_3 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:42,832 - core.experiment_manager - INFO - Created participant: Agent_4 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,832 - core.experiment_manager - INFO - Created participant: Agent_5 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,833 - core.experiment_manager - INFO - Starting Phase 1 for experiment 0f38958d-e145-48d9-b289-b8957f82f646\n",
      "2025-08-10 23:21:42,833 - utils.experiment_runner - INFO - Starting experiment: config_08\n",
      "2025-08-10 23:21:42,836 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,837 - core.experiment_manager - INFO - Created participant: Agent_1 (meta/llama/llama-4-maverick, temp=0.0)\n",
      "2025-08-10 23:21:42,837 - core.experiment_manager - INFO - Created participant: Agent_2 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:42,837 - core.experiment_manager - INFO - Created participant: Agent_3 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 23:21:42,837 - core.experiment_manager - INFO - Created participant: Agent_4 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,838 - core.experiment_manager - INFO - Created participant: Agent_5 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,838 - core.experiment_manager - INFO - Starting Phase 1 for experiment 809379c1-4f85-49fe-ad06-e4e45cc10db3\n",
      "2025-08-10 23:21:42,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,851 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,851 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,853 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= meta/llama/llama-4-maverick; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,856 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,857 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,858 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:42,927 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:42,938 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_06.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:42,939 - utils.experiment_runner - INFO - Starting experiment: config_09\n",
      "2025-08-10 23:21:42,942 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:42,942 - core.experiment_manager - INFO - Created participant: Agent_1 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,942 - core.experiment_manager - INFO - Created participant: Agent_2 (moonshotai/kimi-k2, temp=0.0)\n",
      "2025-08-10 23:21:42,943 - core.experiment_manager - INFO - Created participant: Agent_3 (openai/gpt-4.1, temp=0.0)\n",
      "2025-08-10 23:21:42,943 - core.experiment_manager - INFO - Created participant: Agent_4 (openai/gpt-4.1-mini, temp=0.0)\n",
      "2025-08-10 23:21:42,943 - core.experiment_manager - INFO - Created participant: Agent_5 (anthropic/claude-sonnet-4, temp=0.0)\n",
      "2025-08-10 23:21:42,943 - core.experiment_manager - INFO - Starting Phase 1 for experiment 2c8ad66b-0f40-4840-a677-83279d5a9bfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:42,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:42,955 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,060 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-08-10 23:21:43,069 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_08.yaml: Phase 1 execution failed: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"meta/llama/llama-4-maverick is not a valid model ID\",\"code\":400},\"user_id\":\"user_2x01OHGIKmuCBHUlfjGarb443gH\"}\n",
      "2025-08-10 23:21:43,070 - utils.experiment_runner - INFO - Starting experiment: config_10\n",
      "2025-08-10 23:21:43,075 - utils.language_manager - INFO - Language set to: English\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_1 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_2 (google/gemini-2.5-flash, temp=0.0)\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_3 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_4 (google/gemini-2.5-pro, temp=0.0)\n",
      "2025-08-10 23:21:43,077 - core.experiment_manager - INFO - Created participant: Agent_5 (deepseek/deepseek-chat-v3-0324, temp=0.0)\n",
      "2025-08-10 23:21:43,078 - core.experiment_manager - INFO - Starting Phase 1 for experiment abee0236-ba1d-4b1d-9c8c-251783b52122\n",
      "2025-08-10 23:21:43,079 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,081 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,081 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:43,082 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:21:43,111 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,195 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,262 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,274 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,305 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,391 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,410 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,436 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,439 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,468 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,478 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,479 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,483 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,484 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,532 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,564 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,626 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,627 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,722 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,730 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,850 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,901 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,939 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:43,991 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,006 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,034 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,068 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,201 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,263 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,441 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:44,546 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:45,238 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:45,287 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:45,306 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:45,307 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:45,341 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:45,487 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:45,746 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:45,874 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,026 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:46,089 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,155 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,270 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,305 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:46,397 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,477 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,515 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,528 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:21:46,543 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:46,593 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:46,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:46,865 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:46,940 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:46,958 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:46,979 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:47,045 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:47,176 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:47,251 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:47,362 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:47,414 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:47,557 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:47,607 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:47,805 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:47,931 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:47,939 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:21:48,090 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:48,143 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:48,437 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,467 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:48,526 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,818 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,874 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:48,953 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:49,053 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,065 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:49,348 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:49,429 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:49,516 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,586 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:49,599 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,613 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,940 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:49,992 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:50,098 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:50,189 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,396 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,402 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:50,468 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,608 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,639 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:50,903 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:50,979 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:50,988 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:51,008 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:51,033 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:51,089 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:51,120 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:51,424 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:51,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:51,587 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:51,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:51,819 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:51,939 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:52,007 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,053 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,240 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,305 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:21:52,313 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:52,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,529 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:52,827 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,938 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:52,948 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:21:52,983 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:53,286 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,350 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,393 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,456 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,456 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:21:53,554 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,571 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,574 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,741 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:53,755 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:21:53,880 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,006 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,126 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,170 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,265 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,435 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,560 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:21:54,704 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,713 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,731 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:54,846 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:21:54,909 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:54,918 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,021 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,087 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,261 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,265 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,376 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,637 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:55,822 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:55,851 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,041 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,054 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,076 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,248 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,407 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:56,453 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,474 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,496 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,618 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,732 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:56,821 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:56,923 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:56,939 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:57,157 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:57,216 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,216 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:57,271 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:57,450 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:21:57,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,547 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,600 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,606 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:57,840 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,397 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,431 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,597 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:21:58,710 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:58,772 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:21:58,776 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:58,870 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:59,152 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:59,334 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:59,415 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:21:59,594 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:21:59,814 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:21:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:21:59,848 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,355 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,532 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:00,536 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,540 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:00,682 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:00,765 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,822 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,823 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:00,850 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,884 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:00,946 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:00,960 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:01,126 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,170 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,328 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,336 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,354 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:01,467 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:01,689 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:01,956 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:02,086 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:02,277 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:02,494 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:02,779 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:02,801 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:02,893 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,224 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,563 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,586 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:03,586 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:03,700 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,799 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,825 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:03,828 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,860 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:03,884 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:03,968 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,000 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,151 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:04,162 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,181 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,253 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,289 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:04,291 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,292 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,387 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:04,517 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:04,772 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,831 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,860 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:04,933 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,020 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:05,135 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:05,154 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:05,214 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,313 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,493 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,593 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,641 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:05,651 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:05,769 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:05,792 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:05,936 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,194 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,269 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,273 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,326 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,439 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,497 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,497 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,517 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:06,593 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,648 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,680 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,731 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,786 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:06,798 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,868 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:06,921 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:06,949 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,043 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,091 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,095 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:07,196 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:07,274 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:07,330 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:07,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:07,575 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,817 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,828 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:07,835 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:07,969 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:08,002 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:08,119 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:08,338 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:08,359 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:08,373 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:08,401 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:08,414 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:08,710 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:08,732 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:08,933 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:09,061 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:09,177 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:09,349 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:09,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:09,372 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:09,727 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:09,815 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:09,904 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,038 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,114 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:10,126 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,169 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,343 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,387 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:10,414 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,429 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:10,514 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,714 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:10,797 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:10,837 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,906 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,923 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:10,929 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:11,008 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:11,210 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:11,257 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:11,428 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:11,487 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:11,736 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:11,902 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:11,909 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:12,113 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,119 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,120 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,216 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:12,320 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,323 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:12,421 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,423 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:12,628 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:12,629 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:12,839 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:12,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,845 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:12,845 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:12,942 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,044 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,261 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,266 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:13,313 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,319 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,341 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:13,391 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,402 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,538 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:13,612 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,723 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,818 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:13,860 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:13,952 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,000 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:14,002 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,075 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,253 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,461 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:14,522 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:14,527 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:14,630 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:14,651 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:14,654 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:14,676 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:14,850 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,101 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,214 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,268 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,278 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:15,367 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:15,637 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,654 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,666 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,787 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:15,790 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,792 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:15,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,882 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:15,908 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:16,074 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,175 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,273 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,312 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,337 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:16,348 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,453 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,505 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,514 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,574 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,659 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:16,716 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:16,792 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,802 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,861 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,925 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,930 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:16,989 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,421 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:17,588 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,661 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,825 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:17,827 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,839 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:17,869 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:18,061 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:18,081 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:18,087 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,139 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:18,149 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:18,262 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,277 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,377 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,423 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,431 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:18,519 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:19,386 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:19,783 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:19,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:19,796 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:19,968 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,147 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:20,149 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:20,166 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:20,248 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,275 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,299 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,323 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:20,443 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,484 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:20,748 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:20,789 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:20,812 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:20,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:20,958 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:21,284 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,294 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:21,498 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,551 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:21,670 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:21,703 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:21,811 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:21,870 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:21,997 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:22,002 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:22,025 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,396 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,560 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,697 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,824 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:22,939 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:22,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:22,967 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,078 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:23,130 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:23,238 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:23,239 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:23,385 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,521 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,558 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:23,676 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:23,900 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:23,964 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,189 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,447 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,453 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:24,471 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:24,549 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,782 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,852 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:24,889 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,956 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:24,990 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,020 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:25,022 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,070 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,141 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,226 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:25,266 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,275 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,307 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,455 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,493 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,497 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:25,557 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:25,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,841 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,895 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:25,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,164 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:26,169 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,172 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:26,470 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:26,497 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,636 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,751 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:26,828 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:26,844 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:26,953 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:27,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,047 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,254 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,289 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:27,318 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,322 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,332 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:27,434 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:27,513 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,546 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,709 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,710 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,755 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:27,910 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:28,122 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,166 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,177 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:28,239 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,271 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,445 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:28,671 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:28,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:28,880 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:28,936 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,365 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:29,395 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,578 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:29,699 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,917 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:29,996 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:30,035 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,167 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,197 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,207 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,411 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,422 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,422 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:30,427 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,478 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:30,562 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,588 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:30,627 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:30,631 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,761 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:30,915 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:31,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,148 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:31,175 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,240 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:31,337 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,442 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,764 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:31,803 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:31,823 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,929 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:31,946 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:32,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:32,059 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:32,119 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,165 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,409 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,421 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,680 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:32,728 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,030 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,035 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:33,529 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,534 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,649 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,779 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:33,891 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:33,951 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,051 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:34,068 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:34,159 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,226 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:34,504 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,556 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,590 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,603 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:34,647 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:34,759 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:34,786 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:34,837 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:34,955 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:35,016 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:35,031 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:35,184 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:35,329 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:35,370 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:35,374 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:35,724 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:35,880 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:35,892 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:35,963 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,209 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,280 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:36,415 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,544 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:36,617 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:36,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,703 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:36,715 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:37,064 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:37,190 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:37,512 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:37,567 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:38,014 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:38,079 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:38,469 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:38,709 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:38,997 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:39,327 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,373 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:39,420 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,465 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,574 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,705 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:39,869 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,882 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:39,956 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:39,972 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:40,003 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:40,158 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:40,421 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:40,791 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:40,813 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:40,815 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,077 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,137 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:41,191 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:41,284 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,348 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:41,361 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,451 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,455 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,463 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:41,532 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:41,556 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:41,670 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:41,682 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:41,734 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:41,808 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:42,009 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:42,064 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,065 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,072 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,295 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:42,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,540 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:42,698 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:42,775 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:42,835 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:43,305 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:43,600 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:43,702 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:43,838 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:43,958 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,005 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,031 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:44,204 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:44,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,278 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:44,530 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:44,558 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,636 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:44,815 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n",
      "2025-08-10 23:22:44,871 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:45,080 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,195 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,234 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:45,331 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,483 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:45,662 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,731 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,792 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:45,874 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1-mini; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:46,047 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:46,142 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:46,171 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:46,196 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:46,451 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:46,459 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:46,598 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:46,760 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:46,785 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:46,918 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:47,021 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:47,468 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:47,524 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:47,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:47,986 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:48,145 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:48,349 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:48,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:49 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:49,841 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:50,083 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:50,251 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:50,461 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:50,567 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:50,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:50,777 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,104 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,113 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,626 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,827 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:51,895 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:52,184 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:52,288 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:52,363 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:52,372 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:52,493 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:52,613 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:52,638 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:52,728 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:22:52,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:53,029 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:53,055 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:22:53,255 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:53,429 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:53,462 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:53,746 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:53,854 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:54,005 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:54,029 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:54,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:54,484 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:54,603 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:54,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:54,718 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:55,002 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:55,091 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:55,276 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:55,369 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:55,772 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:56,467 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:22:56,834 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:56,860 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:57,173 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:22:57,179 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:57,366 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:58,023 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:58,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:58,300 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:22:58,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:58,439 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:22:58,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:59,273 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:59,468 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:59,796 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:59,800 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:22:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:22:59,845 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:22:59,993 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:59,995 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:22:59,995 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:00,037 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:00,083 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:00,151 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:00,321 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:00,398 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:00,522 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:00,526 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:00,855 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:00,933 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:01,433 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:01,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:01,895 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:02,092 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:02,302 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:02,335 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:02,730 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:02,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:03,213 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:03,287 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:03,466 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:03,661 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:03,663 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:04,075 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:04,186 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:04,326 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:04,360 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:05,761 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:05,848 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:05,881 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:06,092 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:06,460 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:06,609 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:06,611 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:06,650 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:06,669 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:07,199 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:07,334 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:07,665 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:07,695 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:07 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:07,715 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:07,855 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:08,185 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:08,197 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:08,285 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:09,234 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:09,269 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:11,375 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:11,531 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:11,688 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:11,955 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:12,085 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:12,346 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:12,635 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:13,292 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n",
      "2025-08-10 23:23:13,557 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:13,576 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:13,824 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:13,833 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:14,106 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:14,180 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:14,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:14,625 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:14,670 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:15,006 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:15,256 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:15,360 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:15,647 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:17,075 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:17,766 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:17,899 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:18,057 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:18,514 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:18,533 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:19,377 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:19,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:19,487 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:19,528 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:19,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:19,937 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:20,969 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:21,177 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:21,529 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:21,787 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:23,000 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:23,203 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:23,228 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:23,384 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:23:23,500 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:23,576 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:23,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:24,215 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:24,295 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:24,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:24,827 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:24,940 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:25,004 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:25,316 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:25,497 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:26,325 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:26,378 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:26,412 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:27,340 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:28,151 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,078 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,215 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,508 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,680 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:29,835 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:29,881 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:29,885 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4.1; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:30,157 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:30,312 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:30,870 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:31,067 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:31,853 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:31,867 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:32,285 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:32,443 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:33,012 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:34,180 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:34,363 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:34,453 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:34,460 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:34,516 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:35,043 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:35,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= qwen/qwen3-coder; provider = openrouter\n",
      "2025-08-10 23:23:35,484 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:35,669 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:35,743 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:35,869 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:36,193 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:36,292 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:36,594 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:36,640 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:37,064 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:37,578 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:38,020 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:38,737 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:39,038 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:39,279 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:39,433 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:40,469 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:40,878 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:41,965 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:42,585 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:43,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:44,609 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:44,860 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:45,585 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:45,732 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:47,371 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:47,419 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:47,487 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:47,761 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:48,145 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:48,238 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:23:49,340 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:49,692 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:49,805 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:50,332 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:50,675 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:51,018 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:51,401 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:23:52,095 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:52,289 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:52,619 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:52 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:52,969 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:53,431 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:53,599 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:23:53,959 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:54,146 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:23:55,018 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:57,074 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:58,955 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:23:59,370 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:23:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:23:59,607 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:00,364 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:00,491 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:00,620 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:01,482 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:01,882 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:02,809 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:05,014 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:05,279 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:05,400 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:05,698 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,087 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,393 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,412 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,418 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,601 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,767 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:06,857 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:06,884 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:06,956 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:07,037 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:07,446 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:07,602 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:08,088 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:08,271 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:09,149 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:09,542 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:11,124 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:12,268 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:12,366 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:12,526 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:12,597 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:12,936 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:13,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:13,292 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:13,638 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:13 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:13,884 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:14,454 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:14,472 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:14,502 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:14,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:14,889 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:15,016 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:16,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:16,232 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:16,368 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:16,656 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:16 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:16,741 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:16,901 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:17,204 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:17,418 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:17,858 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:17,961 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:17,968 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:19,087 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:19,846 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:20,404 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:20,576 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:20,806 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:22,071 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:24:22,818 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:24,104 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:24,680 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:25,259 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:25,583 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:26,013 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:26,137 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:26,362 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:26,403 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:26,500 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:26 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:26,587 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:26,901 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:27,187 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:27,503 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:27,521 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:27,735 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:27,760 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:28,191 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:28,955 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:29,115 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:29,169 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:29,702 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:29,750 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:30,157 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:30,485 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:30,699 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:32,232 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:34,303 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:34,733 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:35,086 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:35,329 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:35,749 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:35,908 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:35,981 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:36,295 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:37,100 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:37 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:37,647 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:38,182 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:38,973 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:40,253 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:40,603 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:40,802 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:41,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:41,708 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:41,826 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:42,096 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:42,142 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:42,767 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:43,267 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:43,504 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:44,367 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:44,696 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:45,758 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:46,798 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:46,897 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:46,898 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:48,518 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:50,227 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:50,546 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:24:50,807 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:51,423 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:24:52,153 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:53,039 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:53,099 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:24:53,998 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:57,390 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:57,776 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:24:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:24:58,435 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:24:59,029 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:24:59,260 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:00,453 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:25:01,157 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:01,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:01,316 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:01 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:01,729 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:02,131 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:02,486 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:25:02,976 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:03,379 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:03,748 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:04,159 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:05,814 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:06,502 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:06,881 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:07,309 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:08,015 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:08,339 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:08,472 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:08,989 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:08,994 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:10 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:10,540 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:25:10,904 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:11,065 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:11,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:11 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:11,927 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:12,644 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:12,668 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:14,310 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:15,851 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:16,030 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:17,364 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:17,445 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:25:17,860 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:19,011 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:19 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:19,527 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:20,517 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:20,983 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:23,221 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:23,798 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:25,341 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:25,832 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:26,598 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:28,039 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:29 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:29,133 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:29,702 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:30,137 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:30,948 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:31 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:31,486 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:31,556 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:32,904 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:33,236 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:33,574 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:34,729 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:35,206 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:36,077 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:36,500 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:38 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:38,550 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:38,725 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:39,276 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:40,692 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:40,895 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:43 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:43,496 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:45,072 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:46,241 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:46,610 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:25:47,079 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:47 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:47,494 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:48,305 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:48,647 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:48,742 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:25:49,157 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:50,580 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:25:52,999 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:57,092 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:25:57,630 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:58,973 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:25:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:25:59,969 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:00,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:00,593 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:01,154 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:01,665 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:02,632 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:02 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:02,653 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:03,235 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:03,326 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:05,533 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:26:05,535 - core.experiment_manager - INFO - Agent_1: $8.61 earned\n",
      "2025-08-10 23:26:05,535 - core.experiment_manager - INFO - Agent_2: $15.49 earned\n",
      "2025-08-10 23:26:05,535 - core.experiment_manager - INFO - Agent_3: $9.65 earned\n",
      "2025-08-10 23:26:05,536 - core.experiment_manager - INFO - Agent_4: $8.34 earned\n",
      "2025-08-10 23:26:05,537 - core.experiment_manager - INFO - Agent_5: $6.04 earned\n",
      "2025-08-10 23:26:05,538 - core.experiment_manager - INFO - Starting Phase 2 for experiment 2c8ad66b-0f40-4840-a677-83279d5a9bfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:05,542 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:26:05,913 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:06 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:06,301 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= anthropic/claude-sonnet-4; provider = openrouter\n",
      "2025-08-10 23:26:08,178 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:09,743 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:09,766 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:09,988 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:26:10,556 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:12,262 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:12,583 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:12 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:12,865 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:26:15,336 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:18,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:18,493 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:19,241 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:20,836 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:20 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:20,930 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:21,688 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:21,797 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:22,657 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:23,434 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:23,911 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:24,072 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:25 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:25,691 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:26:26,671 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:27,836 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:27,951 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:28 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:28,119 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:28,545 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:28,662 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:32,647 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= moonshotai/kimi-k2; provider = openrouter\n",
      "2025-08-10 23:26:33,449 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:34,298 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:35,069 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:36,781 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:37,477 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:39,645 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:40,410 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:41,019 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:41,319 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:41,879 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:41,984 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:42,054 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:42,608 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:45,665 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:46,799 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:46 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:46,997 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:26:47,490 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:26:47,490 - openai._base_client - INFO - Retrying request to /responses in 0.499911 seconds\n",
      "2025-08-10 23:26:49,078 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:50 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:50,281 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:51,375 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:52,178 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:52,342 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:26:52,343 - openai._base_client - INFO - Retrying request to /responses in 0.901644 seconds\n",
      "2025-08-10 23:26:52,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:53,163 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:55 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:55,693 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:55,949 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:26:56,753 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:57 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:57,689 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:26:58,400 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:26:58,403 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_2250783194f7aef4c9fd434d14f3019a)\n",
      "2025-08-10 23:26:58,403 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_09.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2025-08-10 23:26:58,928 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:26:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:26:59,839 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:00,871 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:02,768 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:03,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:08 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:08,527 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:09,365 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:09,413 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:10,436 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:11,037 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:11,574 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:14 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:14,181 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:14,751 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:15,490 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:15 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:15,786 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:18,560 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:18,864 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:21 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:21,608 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:22,330 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:22 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:22,817 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:24,529 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:24,840 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:27:25,227 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:25,673 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:25,960 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:27 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:27,708 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:30,102 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:30,238 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:33,101 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:33,722 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:34,859 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:35,431 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:36 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:36,507 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:36,910 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:38,895 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:40,488 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:40,987 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:42,707 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:42 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:42,965 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:43,841 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:44,812 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:45,242 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:45,767 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:51,356 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:51 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:51,375 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:52,301 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:53,135 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:27:53,502 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:27:55,621 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:56 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:56,241 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:27:57,061 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:27:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:27:58,814 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:00,136 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:00,887 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_1: $8.29 earned\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_2: $11.06 earned\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_3: $11.88 earned\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_4: $10.02 earned\n",
      "2025-08-10 23:28:00,889 - core.experiment_manager - INFO - Agent_5: $9.07 earned\n",
      "2025-08-10 23:28:00,890 - core.experiment_manager - INFO - Starting Phase 2 for experiment 85dfa257-dff6-41ac-9170-0b4d17beec88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:00,892 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:28:01,470 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:03 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:03,623 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:28:04,156 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:04,745 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:05,303 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:28:05,671 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:07,244 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:15,884 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:28:15,886 - openai._base_client - INFO - Retrying request to /responses in 0.470358 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:17,304 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:18,497 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:22,082 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:28:22,083 - openai._base_client - INFO - Retrying request to /responses in 0.862530 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:23 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:23,865 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:24,368 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:28:24,368 - core.experiment_manager - INFO - Agent_1: $9.25 earned\n",
      "2025-08-10 23:28:24,368 - core.experiment_manager - INFO - Agent_2: $12.24 earned\n",
      "2025-08-10 23:28:24,368 - core.experiment_manager - INFO - Agent_3: $8.19 earned\n",
      "2025-08-10 23:28:24,369 - core.experiment_manager - INFO - Agent_4: $8.77 earned\n",
      "2025-08-10 23:28:24,369 - core.experiment_manager - INFO - Agent_5: $13.91 earned\n",
      "2025-08-10 23:28:24,369 - core.experiment_manager - INFO - Starting Phase 2 for experiment 637e3be6-986a-452f-916b-5853f5539f4e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:24,370 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:28:25,834 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:26,615 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:29,062 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:28:29,063 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_64fbace3aed44361deff328c51e7f332)\n",
      "2025-08-10 23:28:29,064 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_03.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:32 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:32,410 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:34,516 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:39 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:39,291 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:40 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:40,205 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:28:40,205 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:43,042 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:45,457 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:46,639 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:48,388 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:28:50,992 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:53 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:53,628 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:28:54 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:28:54,340 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:28:54,486 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:28:56,746 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:00 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:00,353 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:29:01,468 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:29:15,682 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:29:15,684 - openai._base_client - INFO - Retrying request to /responses in 0.448560 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:17 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:17,913 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:18,877 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:29:18,979 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:29:21,017 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:29:21,508 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:29:21,509 - openai._base_client - INFO - Retrying request to /responses in 0.914989 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:24 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:24,354 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:29:24,882 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:29:28,371 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:29:28,375 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_d9ca697a8b37d6d874278642f9d1cc76)\n",
      "2025-08-10 23:29:28,377 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_01.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:41 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:41,114 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:29:42,508 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:29:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:29:44,288 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:29:45,273 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:05 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:05,695 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:30:06,884 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:09 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:09,947 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek/deepseek-chat-v3-0324; provider = openrouter\n",
      "2025-08-10 23:30:10,245 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:30:30,726 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:30:30,728 - core.experiment_manager - INFO - Agent_1: $11.50 earned\n",
      "2025-08-10 23:30:30,728 - core.experiment_manager - INFO - Agent_2: $9.15 earned\n",
      "2025-08-10 23:30:30,728 - core.experiment_manager - INFO - Agent_3: $9.67 earned\n",
      "2025-08-10 23:30:30,728 - core.experiment_manager - INFO - Agent_4: $11.90 earned\n",
      "2025-08-10 23:30:30,729 - core.experiment_manager - INFO - Agent_5: $14.74 earned\n",
      "2025-08-10 23:30:30,729 - core.experiment_manager - INFO - Starting Phase 2 for experiment abee0236-ba1d-4b1d-9c8c-251783b52122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:30,731 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:30:31,328 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:33 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:33,510 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:30:33,857 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:35 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:35,704 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-flash; provider = openrouter\n",
      "2025-08-10 23:30:36,417 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:30:52,929 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:30:52,931 - openai._base_client - INFO - Retrying request to /responses in 0.441877 seconds\n",
      "2025-08-10 23:30:58,694 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:30:58,695 - openai._base_client - INFO - Retrying request to /responses in 0.995530 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:30:59 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:30:59,861 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:31:00,844 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:31:05,554 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:31:05,560 - openai.agents - ERROR - Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_c76bb928cef7f413b1223af044ecc1c8)\n",
      "2025-08-10 23:31:05,561 - utils.experiment_runner - ERROR - Failed experiment hypothesis_2_&_4/configs/condition_1/config_10.yaml: Phase 2 execution failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:31:44 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:31:44,235 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= x-ai/grok-4; provider = openrouter\n",
      "2025-08-10 23:31:45,349 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:33:30,875 - core.experiment_manager - INFO - Phase 1 completed. 5 participants finished.\n",
      "2025-08-10 23:33:30,878 - core.experiment_manager - INFO - Agent_1: $9.49 earned\n",
      "2025-08-10 23:33:30,878 - core.experiment_manager - INFO - Agent_2: $8.35 earned\n",
      "2025-08-10 23:33:30,879 - core.experiment_manager - INFO - Agent_3: $11.52 earned\n",
      "2025-08-10 23:33:30,880 - core.experiment_manager - INFO - Agent_4: $13.69 earned\n",
      "2025-08-10 23:33:30,880 - core.experiment_manager - INFO - Agent_5: $13.00 earned\n",
      "2025-08-10 23:33:30,881 - core.experiment_manager - INFO - Starting Phase 2 for experiment 0f38958d-e145-48d9-b289-b8957f82f646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:33:30 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:33:30,886 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:33:33,039 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:33:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:33:48,943 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:33:51,543 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:34:04 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-10 23:34:04,197 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= google/gemini-2.5-pro; provider = openrouter\n",
      "2025-08-10 23:34:07,320 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 23:34:27,969 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:34:27,970 - openai._base_client - INFO - Retrying request to /responses in 0.450403 seconds\n",
      "2025-08-10 23:34:33,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-10 23:34:33,529 - openai._base_client - INFO - Retrying request to /responses in 0.883945 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the experiments\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[43mrun_experiments_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_PARALLEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParallel execution completed at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/utils/experiment_runner.py:337\u001b[39m, in \u001b[36mrun_experiments_parallel\u001b[39m\u001b[34m(config_files, max_parallel, output_dir, verbose)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiments_parallel\u001b[39m(\n\u001b[32m    320\u001b[39m     config_files: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    321\u001b[39m     max_parallel: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    322\u001b[39m     output_dir: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mhypothesis_2_&_4/logs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    323\u001b[39m     verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    324\u001b[39m ) -> List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    325\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    Synchronous wrapper for running experiments in parallel.\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    335\u001b[39m \u001b[33;03m    - List of experiment results dictionaries\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_experiments_parallel_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Githubg/Rawls_v3/.venv/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28mself\u001b[39m._selector.control(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Set up execution parameters\n",
    "MAX_PARALLEL = 1  # Number of experiments to run concurrently\n",
    "OUTPUT_DIR = \"hypothesis_2_&_4/logs\"  # Directory to save results\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Starting parallel execution of {len(config_files)} experiments\")\n",
    "print(f\"Maximum parallel experiments: {MAX_PARALLEL}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run the experiments\n",
    "try:\n",
    "    results = run_experiments_parallel(\n",
    "        config_files=config_files,\n",
    "        max_parallel=MAX_PARALLEL,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Parallel execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nParallel execution failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    results = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
